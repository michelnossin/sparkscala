{
  "paragraphs": [
    {
      "text": "%md\n\n### This notebook will show location data of flights on a animated map. Also using Spark ML predictions will be made for landing time/in block time in minutes. Then we will create an online/realtime prediction using this model.\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:45:13 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eThis notebook will show location data of flights on a animated map. Also using Spark ML predictions will be made for landing time/in block time in minutes. Then we will create an online/realtime prediction using this model.\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517162898799_-276824224",
      "id": "20180128-180818_871793793",
      "dateCreated": "Jan 28, 2018 6:08:18 PM",
      "dateStarted": "Feb 4, 2018 12:45:13 PM",
      "dateFinished": "Feb 4, 2018 12:45:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nspark.version\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 7:20:35 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "res83: String \u003d 2.2.0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517167226896_-1968109593",
      "id": "20180128-192026_970087545",
      "dateCreated": "Jan 28, 2018 7:20:26 PM",
      "dateStarted": "Jan 28, 2018 7:20:35 PM",
      "dateFinished": "Jan 28, 2018 7:20:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Mount your flight fs with the sample data to your docker container",
      "text": "%sh\n\n#docker run -p 8080:8080 -it -v C:\\Users\\miche\\Downloads\\flights:/tmp/somedir cc29eb143bda\nls /tmp/somedir\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 6:08:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "all.csv\nams.csv\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517153994392_-1210723578",
      "id": "20180128-153954_572750284",
      "dateCreated": "Jan 28, 2018 3:39:54 PM",
      "dateStarted": "Jan 28, 2018 4:06:42 PM",
      "dateFinished": "Jan 28, 2018 4:06:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load in the dataset",
      "text": "%spark\r\nimport org.apache.spark.sql.Encoders\r\n\r\nval track_file \u003d \"/tmp/somedir/all.csv\"\r\n\r\ncase class FlightEntry(na1 : String, altitude : String, dest : String, heading:String, flight : String ,fltid : String, landed : String, time : String ,\r\n     lat : String, lon : String,na3: String,org: String ,na4:String,registration:String,flight2:String,speed :String,na6:String,planetype:String, altitude_delta:String)\r\nval tracksDf \u003d spark.read.option(\"header\", false).option(\"inferSchema\", false).schema(Encoders.product[FlightEntry].schema).csv(track_file).filter($\"dest\" \u003d\u003d\u003d \"AMS\") //dest Amsterdam only\r\nval tracksMinuteDf \u003d tracksDf.withColumn(\"minute\", from_unixtime($\"time\",\"YYYY-MM-dd HH:mm\") )\r\nval flightMapDf \u003d tracksMinuteDf.select($\"flight\",$\"minute\",$\"lat\",$\"lon\").sort(col(\"minute\").asc) \r\nval flightArray \u003d flightMapDf.collect",
      "user": "anonymous",
      "dateUpdated": "Jan 29, 2018 4:07:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.Encoders\ntrack_file: String \u003d /tmp/somedir/all.csv\ndefined class FlightEntry\ntracksDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [na1: string, altitude: string ... 17 more fields]\ntracksMinuteDf: org.apache.spark.sql.DataFrame \u003d [na1: string, altitude: string ... 18 more fields]\nflightMapDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [flight: string, minute: string ... 2 more fields]\nflightArray: Array[org.apache.spark.sql.Row] \u003d Array([KL214,2016-06-23 23:31,49.2658,-121.8836], [KL282,2016-06-23 23:31,47.2353,-108.7284], [DL142,2016-06-23 23:31,52.2732,-111.6387], [KL610,2016-06-23 23:31,40.9074,-112.0103], [DL56,2016-06-23 23:31,40.7745,-111.9742], [KL602,2016-06-23 23:31,49.092,-98.3777], [DL178,2016-06-23 23:31,55.0896,-94.5454], [UA909,2016-06-23 23:31,42.308,-85.4874], [KL678,2016-06-23 23:31,57.9245,-89.9364], [UA20,2016-06-23 23:31,39.1989,-86.4969], [KL662,2016-06-23 23:31,41.4086,-70.2907], [OR338,2016-06-23 23:31,39.9246,-73.3105], [MP6332,2016-06-23 23:31,40.1743,-72.0937], [KL612,2016-06-23 23:31,47.8331,-70.7206], [KL692,2016-06-23 23:31,49.8256,-62.093], [UA946,2016-06-23 23:31,45.2445,-66.4383], [UA70,2016-06-23 23:31,44.9011,-64.4659], [DL72,2016-06..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517151597051_1096014120",
      "id": "20180128-145957_1452925951",
      "dateCreated": "Jan 28, 2018 2:59:57 PM",
      "dateStarted": "Jan 29, 2018 4:07:15 PM",
      "dateFinished": "Jan 29, 2018 4:08:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Custom spark streaming receiver to simulate the flight events (structured streaming would load csv in 1 blast so not usable)",
      "text": "%spark\r\n\r\nimport org.apache.spark.streaming.receiver.Receiver\r\nimport org.apache.spark.internal.Logging\r\nimport org.apache.spark.storage.StorageLevel\r\nimport scala.util.Random\r\n\r\nclass FlightReplyData(timeBreakSec : Double) extends org.apache.spark.streaming.receiver.Receiver[String](org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_2) { \r\n\r\n  def onStart() {\r\n    // Start the thread that receives data over a connection\r\n    new Thread(\"Socket Receiver Flight\") {\r\n      override def run() { receive() }\r\n    }.start()\r\n  }\r\n\r\n  def onStop() {\r\n   // There is nothing much to do as the thread calling receive()\r\n   // is designed to stop by itself isStopped() returns false\r\n  }\r\n\r\n  /** Create a socket connection and receive data until receiver is stopped */\r\n  private def receive() {\r\n   var lastTime \u003d \"\"\r\n   var resultArray \u003d List[String]()\r\n   while(!isStopped()) {     \r\n        //Lets create an array to return as stream per minute (as our set is ordered by minute (event time)\r\n        flightArray.foreach(r \u003d\u003e {\r\n            resultArray \u003d  (r(0) + \",\" + r(1) + \",\" + r(2) + \",\" + r(3) ).toString :: resultArray\r\n            \r\n            if (r(1).toString !\u003d lastTime) {\r\n                lastTime \u003d r(1).toString\r\n                store(resultArray.reverse.iterator) //reverse to start with oldest time\r\n                resultArray \u003d List[String]()\r\n                Thread.sleep((timeBreakSec * 1000).toLong) //break to give stream some air\r\n            }\r\n            \r\n        }) \r\n       \r\n    }\r\n  }\r\n}\r\n\r\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 7:21:47 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.streaming.receiver.Receiver\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.storage.StorageLevel\nimport scala.util.Random\ndefined class FlightReplyData\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152702874_-853132554",
      "id": "20180128-151822_917904560",
      "dateCreated": "Jan 28, 2018 3:18:22 PM",
      "dateStarted": "Jan 28, 2018 4:08:00 PM",
      "dateFinished": "Jan 28, 2018 4:08:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Plot flights on map (will start in next section)",
      "text": "%angular\n\n\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://unpkg.com/leaflet@1.2.0/dist/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"mapt3\" style\u003d\"height: 800px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027mapt3\u0027).setView([30.00, -30.00], 3);\n    //L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027).addTo(map);\n    //L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003ds\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n    //maxZoom: 20 ,subdomains:[\u0027mt0\u0027]\n    //}).addTo(map);\n    L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003dp\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n        maxZoom: 20,\n        subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]\n    }).addTo(map);\n\n                \n    //subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]            \n    var geoMarkers \u003d L.layerGroup().addTo(map); //Ship icons to be stored in layer, so we can clear all of them each update\n\n    var el \u003d angular.element($(\u0027#mapt3\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027planelocations\u0027, function(newValue, oldValue) {\n            geoMarkers.clearLayers(); //Remove the ship icons at their current location.\n           \n            var isTimeSet \u003d false\n            \n            angular.forEach(newValue, function(event) {\n                if (event) {\n                     if (isTimeSet \u003d\u003d false) {\n                        //lets add time/minute in middle of map\n                            var divIcon \u003d L.divIcon({ \n                            html: \u0027\u003cspan style\u003d\"color:white;font-size: 15pt\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[3] + \u0027\u003c/span\u003e\u0027\n                        });\n                        L.marker(new L.LatLng(28.45900, -45.7509), {icon: divIcon }).addTo(geoMarkers);\n                        isTimeSet \u003d true;\n                    }\n            \n                    \n                    new L.Marker([event.values[1], event.values[2]], {\n                        icon: new L.DivIcon({\n                        className: \u0027my-div-icon\u0027,\n                        html: \u0027\u003cimg width\u003d\"10px\" height\u003d\"10px\" class\u003d\"my-div-image\" src\u003d\"http://www.planetoftunes.com/dtp/png_test/red_circle.png\"/\u003e\u0027+\n                        \u0027\u003cspan style\u003d\"color:DarkGreen\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[0] + \u0027\u003c/span\u003e\u0027\n                    })}).addTo(geoMarkers);\n\n                }\n                   \n            });\n            isTimeSet \u003d false;\n            \n        })\n    });\n}\n\nif (window.locationWatcher) { window.locationWatcher(); }\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://unpkg.com/leaflet@1.2.0/dist/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 4:06:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/undefined",
        "title": true,
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "ANGULAR",
            "data": "\u003clink rel\u003d\"stylesheet\" href\u003d\"https://unpkg.com/leaflet@1.2.0/dist/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"mapt3\" style\u003d\"height: 800px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027mapt3\u0027).setView([30.00, -30.00], 3);\n    //L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027).addTo(map);\n    //L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003ds\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n    //maxZoom: 20 ,subdomains:[\u0027mt0\u0027]\n    //}).addTo(map);\n    L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003dp\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n        maxZoom: 20,\n        subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]\n    }).addTo(map);\n\n                \n    //subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]            \n    var geoMarkers \u003d L.layerGroup().addTo(map); //Ship icons to be stored in layer, so we can clear all of them each update\n\n    var el \u003d angular.element($(\u0027#mapt3\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027planelocations\u0027, function(newValue, oldValue) {\n            geoMarkers.clearLayers(); //Remove the ship icons at their current location.\n           \n            var isTimeSet \u003d false\n            \n            angular.forEach(newValue, function(event) {\n                if (event) {\n                     if (isTimeSet \u003d\u003d false) {\n                        //lets add time/minute in middle of map\n                            var divIcon \u003d L.divIcon({ \n                            html: \u0027\u003cspan style\u003d\"color:white;font-size: 15pt\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[3] + \u0027\u003c/span\u003e\u0027\n                        });\n                        L.marker(new L.LatLng(28.45900, -45.7509), {icon: divIcon }).addTo(geoMarkers);\n                        isTimeSet \u003d true;\n                    }\n            \n                    \n                    new L.Marker([event.values[1], event.values[2]], {\n                        icon: new L.DivIcon({\n                        className: \u0027my-div-icon\u0027,\n                        html: \u0027\u003cimg width\u003d\"10px\" height\u003d\"10px\" class\u003d\"my-div-image\" src\u003d\"http://www.planetoftunes.com/dtp/png_test/red_circle.png\"/\u003e\u0027+\n                        \u0027\u003cspan style\u003d\"color:DarkGreen\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[0] + \u0027\u003c/span\u003e\u0027\n                    })}).addTo(geoMarkers);\n\n                }\n                   \n            });\n            isTimeSet \u003d false;\n            \n        })\n    });\n}\n\nif (window.locationWatcher) { window.locationWatcher(); }\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://unpkg.com/leaflet@1.2.0/dist/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152718761_34513807",
      "id": "20180128-151838_702919106",
      "dateCreated": "Jan 28, 2018 3:18:38 PM",
      "dateStarted": "Jan 28, 2018 4:02:47 PM",
      "dateFinished": "Jan 28, 2018 4:02:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Start Spark stream and watch the previous map",
      "text": "%spark\n\n//No Structured Streaming in Spark 2.1, so we\u0027ll do some Rdd stuff. As spark 2.2 and Zeppelin have an issue at this moment.http://localhost:8080/#/interpreter\n// it seems 2.1 has beta version of structured streaming, still we will use rdd\n\nimport org.apache.spark.streaming.{Milliseconds,Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.expressions.Window\n\nz.angularUnbind(\"planelocations\") //Our geo map in the previous section will be updating  once we update this Zeppelin context variable\n\nStreamingContext.getActive.foreach { _.stop(stopSparkContext \u003d false) }\nval ssc \u003d new StreamingContext(sc, Milliseconds(500)) //Our stream will update each second \nval flightDStream \u003d ssc.receiverStream(new FlightReplyData(0.4)) \n\n//val windowFlightDStream \u003d flightDStream\n\n// Use a Window Dstream with location of couple of runs . Required if you want to aggregate over a longer historical period or prevent duplicates/or dissapearing planes. \nval windowFlightDStream \u003d flightDStream.window(Milliseconds(1000),Milliseconds(500)) \n\n//Get the latest location of each plane (otherwise we would see double, or missings planes on the map). \n//and return it to our Leaflet using a Zepplin Variable named locations (Expecting Array of arrays Name,lat,lon)                                                        \nwindowFlightDStream.foreachRDD { rdd \u003d\u003e  \n\n                        val inputDf \u003d rdd.map (s \u003d\u003e s.split(\",\"))\n                                    .map(arr \u003d\u003e (arr(0),arr(1),arr(2),arr(3)) )\n                                    .toDF(\"id\",\"timest\",\"lat\",\"lon\").na.drop\n                        //option 1 just have a quick but flickering, missing animation\n                        //z.angularBind(\"planelocations\", inputDf.select($\"id\",$\"lat\",$\"lon\",$\"timest\").collect)\n                        \n                        //option 2 group by plane and get the latest location\n                        //val maxTimeStampDf \u003d inputDf.groupBy($\"id\").agg(max($\"timest\") as \"maxtimestamp\").withColumnRenamed(\"id\", \"id2\")\n                        //val latestLocationDf \u003d inputDf.join(maxTimeStampDf,inputDf(\"id\") \u003d\u003d\u003d maxTimeStampDf(\"id2\")).filter($\"timest\" \u003d\u003d\u003d $\"maxtimestamp\")  //.orderBy($\"id\".asc,$\"timestamp\".asc).cache\n                        //z.angularBind(\"planelocations\", latestLocationDf.select($\"id\",$\"lat\",$\"lon\",$\"timest\").collect)\n                        \n                        //Option 3 get a minute field from 1 row and filter all rows with it (and make sure the data is in a bit quicker then the stream interval)\n                        val window \u003d Window.partitionBy($\"id\").orderBy($\"timest\".asc)\n                        z.angularBind(\"planelocations\", inputDf.withColumn(\"rank\", row_number().over(window)).where($\"rank\" \u003d\u003d\u003d 1).select($\"id\",$\"lat\",$\"lon\",$\"timest\").collect  ) //.where($\"rank\" \u003d\u003d\u003d 1)\n                        \n                      \n}                       \n                                                        \nssc.start()\n\n// This is to ensure that we wait for some time before the background streaming job starts. \nssc.awaitTerminationOrTimeout(2000)\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 4:06:06 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.expressions.Window\nssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@1b87eba5\nflightDStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] \u003d org.apache.spark.streaming.dstream.PluggableInputDStream@10823fd4\nwindowFlightDStream: org.apache.spark.streaming.dstream.DStream[String] \u003d org.apache.spark.streaming.dstream.WindowedDStream@27958ce\nres25: Boolean \u003d false\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152736961_-1294008821",
      "id": "20180128-151856_600795405",
      "dateCreated": "Jan 28, 2018 3:18:56 PM",
      "dateStarted": "Jan 28, 2018 4:03:07 PM",
      "dateFinished": "Jan 28, 2018 4:03:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create some UDF\u0027s for our Spark ML predictions (should be in Scala due to performance)",
      "text": "%pyspark\n\nimport pyspark.sql.functions as fn\nimport pyspark.sql.types as typ\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import FloatType\nfrom pyspark.sql.types import IntegerType\n\n#Best is not to put udf in pyspark but scala due to performance, but was already created\n\nfrom math import radians, cos, sin, asin, sqrt\n\ndef showMissingDataPercent(df_miss):\n    \u0027show each column and percentage of missing data, 0 - 1 , 0 means no missing data\u0027\n    df_miss.agg(*[\n       (1 - (fn.count(c) / fn.count(\u0027*\u0027))).alias(c + \u0027_missing\u0027)\n       for c in df_miss.columns\n    ]).show()\n\ndef getDFDropMissingRows(df_miss):\n    \u0027Drop rows with any missing column field\u0027\n    return(df_miss.dropna())\n\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n    # convert decimal degrees to radians \n    lon1, lat1, lon2, lat2 \u003d map(radians, [lon1, lat1, lon2, lat2])\n    # haversine formula \n    dlon \u003d lon2 - lon1 \n    dlat \u003d lat2 - lat1 \n    a \u003d sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c \u003d 2 * asin(sqrt(a)) \n    km \u003d 6367 * c\n    return km\n\ndef dist_to_ams(lat,lon):\n    \"\"\"Distance to centre schiphol airport from any lat lon\"\"\"\n    return haversine(float(4.761392), float(52.308871), float(lon),float(lat))\n\ndef closest_runway(lon,lat,heading):\n    \"\"\" Get closest runway at amsterdam based on plane location and heading. return code 1-12 (6 runways * 2 headings)\"\"\"\n    \n    runway_list \u003d [   { \"runway\" : 1, \"lat\" : 52.350202, \"lon\" : 4.710732 , \"heading1\" : 180 , \"heading2\" : 360, \"name\" : \"polderbaan\" },\n    { \"runway\" : 2, \"lat\" : 52.316110, \"lon\" : 4.738369 , \"heading1\" : 180 , \"heading2\" : 360, \"name\" : \"zwanenburgbaan\" },\n    { \"runway\" : 3, \"lat\" : 52.317579, \"lon\" : 4.772186 , \"heading1\" : 90 , \"heading2\" : 270, \"name\" : \"buitenveldertbaan\" },\n    { \"runway\" : 4, \"lat\" : 52.297217, \"lon\" : 4.757938 , \"heading1\" : 60 , \"heading2\" : 240, \"name\" : \"kaagbaan\" },\n    { \"runway\" : 5, \"lat\" : 52.307714, \"lon\" : 4.778881 , \"heading1\" : 180 , \"heading2\" : 360, \"name\" : \"aalsmeerbaan\" },\n    { \"runway\" : 6, \"lat\" : 52.308659, \"lon\" : 4.795361 , \"heading1\" : 40 , \"heading2\" : 220, \"name\" : \"oostbaan\" }\n]\n    \n    smallest_dist_runway \u003d -1\n    smallest_dist \u003d -1\n    for runway in runway_list:\n        dist \u003d haversine(float(runway[\"lon\"]), float(runway[\"lat\"]), float(lon),float(lat))\n        if smallest_dist \u003d\u003d -1 or dist \u003c smallest_dist:\n            smallest_dist \u003d dist\n            smallest_dist_runway \u003d runway\n    \n    angle1 \u003d 180 - abs(abs(heading - smallest_dist_runway[\"heading1\"]) - 180); \n    angle2 \u003d 180 - abs(abs(heading - smallest_dist_runway[\"heading2\"]) - 180); \n    runway_code \u003d smallest_dist_runway[\"runway\"]\n    \n    if angle1 \u003c angle2 :\n        return runway_code\n    return runway_code + 6\n\ndef distance_to_runway(lon,lat,runway):\n    \"\"\" Get distance based on lat lon to runway given its runway id\"\"\"\n    \n    runway_list \u003d [   { \"runway\" : 1, \"lat\" : 52.350202, \"lon\" : 4.710732 , \"heading1\" : 180 , \"heading2\" : 360, \"name\" : \"polderbaan\" },\n    { \"runway\" : 2, \"lat\" : 52.316110, \"lon\" : 4.738369 , \"heading1\" : 180 , \"heading2\" : 360, \"name\" : \"zwanenburgbaan\" },\n    { \"runway\" : 3, \"lat\" : 52.317579, \"lon\" : 4.772186 , \"heading1\" : 90 , \"heading2\" : 270, \"name\" : \"buitenveldertbaan\" },\n    { \"runway\" : 4, \"lat\" : 52.297217, \"lon\" : 4.757938 , \"heading1\" : 60 , \"heading2\" : 240, \"name\" : \"kaagbaan\" },\n    { \"runway\" : 5, \"lat\" : 52.307714, \"lon\" : 4.778881 , \"heading1\" : 180 , \"heading2\" : 360, \"name\" : \"aalsmeerbaan\" },\n    { \"runway\" : 6, \"lat\" : 52.308659, \"lon\" : 4.795361 , \"heading1\" : 40 , \"heading2\" : 220, \"name\" : \"oostbaan\" }\n]\n    #runway code can be 1-12 (6 runways, 2 directions). So 7 means 1 + 6 , 8 means 2 + 6 etc\n    if runway \u003e 6:\n        runway \u003d runway - 6\n        \n    runway_lat \u003d runway_list[runway-1][\"lat\"]\n    runway_lon \u003d runway_list[runway-1][\"lon\"]\n    \n    return haversine(float(lon),float(lat),float(runway_lon),float(runway_lat))\n\n\ndef distance_to_pier(lon,lat,pier):\n    \"\"\" Get distance based on lat lon to pier given its id\"\"\"\n    \n    pier_list \u003d [\n        {\"id\" : 0 , \"pier\" : \"A\" , \"lon\" : 4.753781 , \"lat\" : 52.300381},\n        {\"id\" : 1 , \"pier\" : \"B\" , \"lon\" : 4.759363 , \"lat\" : 52.302362},\n        {\"id\" : 2 , \"pier\" : \"C\" , \"lon\" : 4.766188 , \"lat\" : 52.305380},\n        {\"id\" : 3 , \"pier\" : \"D\" , \"lon\" : 4.771575 , \"lat\" : 52.309147},\n        {\"id\" : 4 , \"pier\" : \"E\" , \"lon\" : 4.767366 , \"lat\" : 52.312182},\n        {\"id\" : 5 , \"pier\" : \"F\" , \"lon\" : 4.761679 , \"lat\" : 52.313040},\n        {\"id\" : 6 , \"pier\" : \"G\" , \"lon\" : 4.755998 , \"lat\" : 52.312574},\n        {\"id\" : 7 , \"pier\" : \"H\" , \"lon\" : 4.754054 , \"lat\" : 52.310135}\n    ]\n        \n    pier_lat \u003d pier_list[pier][\"lat\"]\n    pier_lon \u003d pier_list[pier][\"lon\"]\n    \n    return haversine(float(lon),float(lat),float(pier_lon),float(pier_lat))\n\ndef closest_pier(lon,lat):\n    \"\"\" Get closest pier id based on a (plane) location\"\"\"\n    \n    pier_list \u003d [\n        {\"id\" : 0 , \"pier\" : \"A\" , \"lon\" : 4.753781 , \"lat\" : 52.300381},\n        {\"id\" : 1 , \"pier\" : \"B\" , \"lon\" : 4.759363 , \"lat\" : 52.302362},\n        {\"id\" : 2 , \"pier\" : \"C\" , \"lon\" : 4.766188 , \"lat\" : 52.305380},\n        {\"id\" : 3 , \"pier\" : \"D\" , \"lon\" : 4.771575 , \"lat\" : 52.309147},\n        {\"id\" : 4 , \"pier\" : \"E\" , \"lon\" : 4.767366 , \"lat\" : 52.312182},\n        {\"id\" : 5 , \"pier\" : \"F\" , \"lon\" : 4.761679 , \"lat\" : 52.313040},\n        {\"id\" : 6 , \"pier\" : \"G\" , \"lon\" : 4.755998 , \"lat\" : 52.312574},\n        {\"id\" : 7 , \"pier\" : \"H\" , \"lon\" : 4.754054 , \"lat\" : 52.310135}\n    ]\n\n    smallest_dist_pier \u003d -1\n    smallest_dist \u003d -1\n    for pier in pier_list:\n        dist \u003d haversine(float(pier[\"lon\"]), float(pier[\"lat\"]), float(lon),float(lat))\n        if smallest_dist \u003d\u003d -1 or dist \u003c smallest_dist:\n            smallest_dist \u003d dist\n            smallest_dist_pier \u003d pier\n            \n    return smallest_dist_pier[\"id\"]\n\ndef negative_to_zero(some_number):\n    \u0027\u0027\u0027Make negative number 0, but keep positive values as-is. Return this new value\u0027\u0027\u0027\n    if some_number \u003c 0:\n        return 0\n    return some_number\n\nimport datetime\n\n#TODO SET TO AMSTERDAM TIME\ndef timestamp_to_hour(my_time):\n    \u0027\u0027\u0027Return hour of a given timestamp in UTC, \u0027\u0027\u0027\n    d \u003d datetime.datetime.utcfromtimestamp(my_time)\n    return d.hour\n\n\nudf_func \u003d udf(dist_to_ams, FloatType())\nudf_func2 \u003d udf(closest_runway, IntegerType())\nudf_func3 \u003d udf(negative_to_zero,IntegerType())\nudf_func4 \u003d udf(closest_pier, IntegerType())\nudf_func5 \u003d udf(distance_to_runway,FloatType())\nudf_func6 \u003d udf(distance_to_pier,FloatType())\nudf_func7 \u003d udf(timestamp_to_hour,IntegerType())\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 4:08:11 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1517152804124_-788155962",
      "id": "20180128-152004_75935242",
      "dateCreated": "Jan 28, 2018 3:20:04 PM",
      "dateStarted": "Jan 28, 2018 4:08:11 PM",
      "dateFinished": "Jan 28, 2018 4:08:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Register Spark sql table",
      "text": "%spark\n\ntracksMinuteDf.registerTempTable(\"flights\") //workaround\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 4:09:22 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152813034_166421383",
      "id": "20180128-152013_1903088131",
      "dateCreated": "Jan 28, 2018 3:20:13 PM",
      "dateStarted": "Jan 28, 2018 4:09:22 PM",
      "dateFinished": "Jan 28, 2018 4:09:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Change data types , munging and some data checks",
      "text": "%pyspark\n\n\ntracks_df \u003d spark.sql(\"select * from flights\")\n\ntracks_df \u003dtracks_df.dropDuplicates() \ntracks_df \u003d getDFDropMissingRows(tracks_df)\nshowMissingDataPercent(tracks_df)\n\n#Change some types\ntracks_df \u003d tracks_df.withColumn(\"altitude\",tracks_df[\"altitude\"].cast(typ.IntegerType()))  \ntracks_df \u003d tracks_df.withColumn(\"altitude_delta\",tracks_df[\"altitude_delta\"].cast(typ.IntegerType()))\ntracks_df \u003d tracks_df.withColumn(\"speed\",tracks_df[\"speed\"].cast(typ.IntegerType()))      \ntracks_df \u003d tracks_df.withColumn(\"heading\",tracks_df[\"heading\"].cast(typ.IntegerType()))   \ntracks_df \u003d tracks_df.withColumn(\"lat\",tracks_df[\"lat\"].cast(typ.FloatType()))  \ntracks_df \u003d tracks_df.withColumn(\"lon\",tracks_df[\"lon\"].cast(typ.FloatType())) \ntracks_df \u003d tracks_df.withColumn(\"time\",tracks_df[\"time\"].cast(typ.LongType())) \ntracks_df \u003d tracks_df.withColumn(\"landed\",tracks_df[\"landed\"].cast(typ.IntegerType()))\n\n#Sanity checks and filter\ntracks_df \u003d tracks_df.where(\"landed \u003d\u003d 0 or landed \u003d\u003d 1\")\ntracks_df \u003d tracks_df.where(\"heading \u003e\u003d 0 and heading \u003c 360\")\ntracks_df \u003d tracks_df.where(\"altitude \u003e -100 and altitude \u003c 50000\")\ntracks_df \u003d tracks_df.where(\"lat \u003e\u003d -90 and lat \u003c\u003d 90\")\ntracks_df \u003d tracks_df.where(\"lon \u003e\u003d -180 and lon \u003c\u003d 180\")\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 4:10:35 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "+-----------+----------------+------------+---------------+--------------+-------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+--------------------+---------------+-------------+-----------+-----------------+----------------------+--------------+\n|na1_missing|altitude_missing|dest_missing|heading_missing|flight_missing|fltid_missing|landed_missing|time_missing|lat_missing|lon_missing|na3_missing|org_missing|na4_missing|registration_missing|flight2_missing|speed_missing|na6_missing|planetype_missing|altitude_delta_missing|minute_missing|\n+-----------+----------------+------------+---------------+--------------+-------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+--------------------+---------------+-------------+-----------+-----------------+----------------------+--------------+\n|        0.0|             0.0|         0.0|            0.0|           0.0|          0.0|           0.0|         0.0|        0.0|        0.0|        0.0|        0.0|        0.0|                 0.0|            0.0|          0.0|        0.0|              0.0|                   0.0|           0.0|\n+-----------+----------------+------------+---------------+--------------+-------------+--------------+------------+-----------+-----------+-----------+-----------+-----------+--------------------+---------------+-------------+-----------+-----------------+----------------------+--------------+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152826263_2102893245",
      "id": "20180128-152026_780633702",
      "dateCreated": "Jan 28, 2018 3:20:26 PM",
      "dateStarted": "Jan 28, 2018 4:09:51 PM",
      "dateFinished": "Jan 28, 2018 4:10:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Feature engineering",
      "text": "%pyspark\n\n# 1)Distance to amsterdam (we will not use it if runway and gate is avaiable. But this would be a nice alternative!)\ntracks_df \u003d tracks_df.withColumn(\"distance_to_ams\", \\\n                            udf_func(tracks_df.lat,tracks_df.lon))\n# 2) landing/touchdown timestamp, based on landed column switching from 0 to 1.\n# 3) landing/touchdown runway\n# 3) landings per hour based on event timestamp (BAsed on UTC HOUR)\n# 4) Time till landing (seconds) and (minutes) based on event timestamp versus touchdown timestamp\n\nfrom pyspark.sql.functions import col, lag\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import max,min\n\n#Landing timestamp\nw \u003d Window.partitionBy(\"flight\").orderBy(\"time\")\n\ndiff \u003d col(\"landed\").cast(\"int\") - lag(\"landed\", 1).over(w).cast(\"int\")\ntracks_touchdown_df \u003d tracks_df.select([\"flight\",\"time\",\"lat\",\"lon\",\"heading\",\"landed\",\"registration\"]).withColumn(\"touchdown\", diff)\ntracks_touchdown_df \u003d tracks_touchdown_df.withColumn(\"runway\", \\\n                            udf_func2(tracks_touchdown_df.lon,tracks_touchdown_df.lat,tracks_touchdown_df.heading))\ntracks_touchdown_df \u003d tracks_touchdown_df.where(\"touchdown \u003d\u003d 1\").select(col(\"flight\").alias(\"flight_touchdown\"), \\\n                                                                         col(\"runway\").alias(\"runway_touchdown\"), \\\n                                                                         col(\"time\").alias(\"time_touchdown\") )\n\n#Remove flights with \u003e 1 landings completely\n#tracks_touchdown_df.groupby([\"flight_touchdown\"]).count().where(\"count \u003e 1\").show()\ntracks_touchdown_df \u003d tracks_touchdown_df.where(\"flight_touchdown !\u003d \u0027HV6118\u0027 and flight_touchdown !\u003d \u0027HV5134\u0027 and flight_touchdown !\u003d \u0027KL1412\u0027 and flight_touchdown !\u003d \u0027HV6332\u0027 and flight_touchdown !\u003d \u0027HV5356\u0027 and flight_touchdown !\u003d \u0027HV5314\u0027 and flight_touchdown !\u003d \u0027TP668\u0027 and flight_touchdown !\u003d \u0027HV6146\u0027 \")\n\n\n#landings per hour\ntracks_touchdown_df \u003d tracks_touchdown_df.withColumn(\"hour_touchdown\", udf_func7(col(\"time_touchdown\")))\ntouchdown_sum_df \u003d tracks_touchdown_df.groupby(\"hour_touchdown\").sum().select(col(\"hour_touchdown\").alias(\"hour_touchdown_grp\"),col(\"sum(hour_touchdown)\").alias(\"sum_hour_touchdown\"))\ntracks_touchdown_df \u003d tracks_touchdown_df.join(touchdown_sum_df,tracks_touchdown_df.hour_touchdown \u003d\u003d touchdown_sum_df.hour_touchdown_grp)\n\njoin_df \u003d tracks_df.join(tracks_touchdown_df,tracks_df.flight \u003d\u003d tracks_touchdown_df.flight_touchdown)\njoin_df \u003d join_df.withColumn(\"time_till_landing\",col(\"time_touchdown\") - col(\"time\"))\njoin_df \u003d join_df.where(\"time_till_landing \u003e -7000\") #Make sure we remove flight that are landing for hours already\njoin_df \u003d join_df.withColumn(\"time_till_landing\", udf_func3(join_df.time_till_landing)) #negatives to 0 (ML requirs it) \njoin_df \u003d join_df.withColumn(\"time_till_landing_minutes\",join_df.time_till_landing / 60)\n\n# 5) On/in block lat/lon/time\n# 6) pier on block\n# 7 Time till on block (minutes) based on event versus onblock timestamp\n\nmaxtime_df \u003d join_df.groupby(col(\"flight\").alias(\"flight_maxtime\")).agg(max(\"time\").alias(\"time_onblock\"))\nonblock_df \u003d maxtime_df.join(join_df,(maxtime_df.flight_maxtime \u003d\u003d join_df.flight) \u0026 (maxtime_df.time_onblock \u003d\u003d join_df.time))\nonblock_df \u003d onblock_df.select(col(\"flight\").alias(\"flight_onblock\"), \\\n                               col(\"lat\").alias(\"lat_onblock\"), \\\n                               col(\"lon\").alias(\"lon_onblock\"), \\\n                               col(\"time_onblock\"))\nonblock_df \u003d onblock_df.withColumn(\"pier_onblock\",udf_func4(col(\"lon_onblock\"),col(\"lat_onblock\")) )\njoin_maxtime_df \u003d join_df.join(onblock_df,join_df.flight \u003d\u003d onblock_df.flight_onblock)\njoin_maxtime_df \u003d join_maxtime_df.withColumn(\"time_till_onblock_minutes\",(col(\"time_onblock\") - col(\"time\")) / 60)\njoin_df \u003d join_maxtime_df\n\n# 8 Distance to landing runway\n# 9 distance to pier on-block\njoin_df \u003d join_df.withColumn(\"distance_to_runway\",udf_func5(col(\"lon\"),col(\"lat\"),col(\"runway_touchdown\")))\njoin_df \u003d join_df.withColumn(\"distance_to_pier\",udf_func6(col(\"lon\"),col(\"lat\"),col(\"pier_onblock\")))\n\n#Our main set OR load the csv in the next section, as it take 15 mins +- to execute\nselect_cols \u003d [\u0027flight\u0027,\u0027sum_hour_touchdown\u0027,\u0027time_till_landing_minutes\u0027,\u0027time_till_onblock_minutes\u0027, \\\n                        \u0027distance_to_ams\u0027,\u0027distance_to_runway\u0027,\u0027distance_to_pier\u0027,\u0027altitude\u0027,\u0027speed\u0027, \\\n                        \u0027heading\u0027,\u0027runway_touchdown\u0027,\u0027pier_onblock\u0027,\"lat\",\"lon\",\"minute\"]\nml_df \u003d join_df.select(select_cols)\n\nz.show(ml_df.limit(100))\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 4:15:42 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "flight\tsum_hour_touchdown\ttime_till_landing_minutes\ttime_till_onblock_minutes\tdistance_to_ams\tdistance_to_runway\tdistance_to_pier\taltitude\tspeed\theading\trunway_touchdown\tpier_onblock\tlat\tlon\tminute\nCND518\t576\t11.716666666666667\t16.983333333333334\t47.779232\t48.825027\t47.552303\t6900\t276\t276\t8\t5\t52.5033\t5.39\t2016-06-24 18:12\nCND518\t576\t132.83333333333334\t138.1\t1693.8646\t1695.5869\t1694.1405\t18075\t358\t341\t8\t5\t41.4897\t20.6076\t2016-06-24 16:10\nCND518\t576\t40.25\t45.516666666666666\t407.6324\t409.39084\t407.81836\t38000\t464\t307\t8\t5\t50.5819\t9.9561\t2016-06-24 17:43\nCND518\t576\t126.31666666666666\t131.58333333333334\t1611.3955\t1613.1158\t1611.6736\t28750\t469\t327\t8\t5\t42.0099\t19.8896\t2016-06-24 16:17\nCND518\t576\t49.916666666666664\t55.18333333333333\t543.90466\t545.6634\t544.105\t38000\t463\t308\t8\t5\t49.8143\t11.4671\t2016-06-24 17:33\nCND518\t576\t6.45\t11.716666666666667\t27.356264\t26.715538\t26.892788\t2850\t206\t272\t8\t5\t52.5544\t4.7906\t2016-06-24 18:17\nCND518\t576\t82.73333333333333\t88.0\t996.11957\t997.8694\t996.35583\t38000\t449\t335\t8\t5\t46.8631\t15.7752\t2016-06-24 17:01\nCND518\t576\t31.666666666666668\t36.93333333333333\t287.6006\t289.34982\t287.75043\t38000\t472\t312\t8\t5\t51.308\t8.6223\t2016-06-24 17:52\nCND518\t576\t0.0\t4.183333333333334\t0.97858787\t0.80808926\t0.8155134\t0\t22\t87\t8\t5\t52.3141\t4.7498\t2016-06-24 18:24\nCND518\t576\t41.35\t46.61666666666667\t423.72937\t425.488\t423.91754\t38000\t462\t307\t8\t5\t50.4921\t10.1375\t2016-06-24 17:42\nCND518\t576\t5.35\t10.616666666666667\t22.16456\t21.391014\t21.701672\t2875\t181\t183\t8\t5\t52.5083\t4.7561\t2016-06-24 18:18\nCND518\t576\t133.98333333333332\t139.25\t1704.5012\t1706.2227\t1704.7781\t16200\t357\t341\t8\t5\t41.3923\t20.6505\t2016-06-24 16:09\nCND518\t576\t106.2\t111.46666666666667\t1320.9075\t1322.6406\t1321.171\t38000\t477\t336\t8\t5\t44.3061\t18.0439\t2016-06-24 16:37\nCND518\t576\t114.61666666666666\t119.88333333333334\t1440.0334\t1441.7595\t1440.3055\t38025\t481\t327\t8\t5\t43.325\t18.7552\t2016-06-24 16:29\nCND518\t576\t78.35\t83.61666666666666\t941.3352\t943.0893\t941.5604\t38000\t451\t336\t8\t5\t47.3648\t15.4526\t2016-06-24 17:05\nCND518\t576\t108.25\t113.51666666666667\t1349.0552\t1350.786\t1349.3215\t38000\t475\t336\t8\t5\t44.0593\t18.1919\t2016-06-24 16:35\nCND518\t576\t101.93333333333334\t107.2\t1259.9855\t1261.7218\t1260.2448\t38000\t477\t322\t8\t5\t44.7925\t17.6458\t2016-06-24 16:41\nCND518\t576\t0.0\t0.95\t0.5608672\t1.4268409\t0.19630021\t0\t0\t145\t8\t5\t52.3137\t4.759\t2016-06-24 18:28\nCND518\t576\t89.2\t94.46666666666667\t1079.1495\t1080.8922\t1079.3995\t38000\t463\t336\t8\t5\t46.1195\t16.2562\t2016-06-24 16:54\nCND518\t576\t94.53333333333333\t99.8\t1153.939\t1155.6783\t1154.194\t38000\t466\t322\t8\t5\t45.5573\t16.8184\t2016-06-24 16:49\nCND518\t576\t0.0\t2.1\t0.5608672\t1.4268409\t0.19630021\t0\t0\t326\t8\t5\t52.3137\t4.759\t2016-06-24 18:26\nCND518\t576\t53.13333333333333\t58.4\t589.3888\t591.1473\t589.5924\t38000\t463\t309\t8\t5\t49.5541\t11.9594\t2016-06-24 17:30\nCND518\t576\t30.666666666666668\t35.93333333333333\t273.6697\t275.41568\t273.81226\t38000\t472\t312\t8\t5\t51.3969\t8.4656\t2016-06-24 17:53\nCND518\t576\t105.15\t110.41666666666667\t1306.4703\t1308.2047\t1306.7324\t38000\t477\t336\t8\t5\t44.4333\t17.968\t2016-06-24 16:38\nCND518\t576\t81.58333333333333\t86.85\t981.61755\t983.3685\t981.8511\t38000\t450\t336\t8\t5\t46.9945\t15.6897\t2016-06-24 17:02\nCND518\t576\t55.28333333333333\t60.55\t620.04974\t621.80804\t620.25525\t38000\t465\t309\t8\t5\t49.3781\t12.2886\t2016-06-24 17:28\nCND518\t576\t128.51666666666668\t133.78333333333333\t1641.5063\t1643.228\t1641.783\t25950\t438\t302\t8\t5\t41.8363\t20.1789\t2016-06-24 16:15\nCND518\t576\t80.53333333333333\t85.8\t968.6718\t970.4239\t968.90283\t38000\t450\t336\t8\t5\t47.1133\t15.6143\t2016-06-24 17:03\nCND518\t576\t47.81666666666667\t53.083333333333336\t514.279\t516.0378\t514.47687\t38000\t459\t308\t8\t5\t49.9825\t11.1431\t2016-06-24 17:35\nCND518\t576\t65.85\t71.11666666666666\t771.2079\t772.9651\t771.4206\t38000\t462\t309\t8\t5\t48.4968\t13.8747\t2016-06-24 17:17\nCND518\t576\t13.866666666666667\t19.133333333333333\t66.41066\t67.68088\t66.25754\t9250\t356\t282\t8\t5\t52.4805\t5.6995\t2016-06-24 18:09\nCND518\t576\t138.25\t143.51666666666668\t1734.3442\t1736.0623\t1734.6246\t6050\t230\t146\t8\t5\t41.0919\t20.7245\t2016-06-24 16:05\nCND518\t576\t93.46666666666667\t98.73333333333333\t1139.0099\t1140.7498\t1139.2643\t38000\t467\t322\t8\t5\t45.6647\t16.7002\t2016-06-24 16:50\nCND518\t576\t21.283333333333335\t26.55\t150.71832\t152.33403\t150.73044\t26950\t425\t292\t8\t5\t52.1979\t6.9696\t2016-06-24 18:02\nCND518\t576\t14.933333333333334\t20.2\t76.32822\t77.697975\t76.21386\t11775\t367\t289\t8\t5\t52.447\t5.8636\t2016-06-24 18:08\nCND518\t576\t66.85\t72.11666666666666\t785.349\t787.1062\t785.5621\t38000\t461\t309\t8\t5\t48.4161\t14.0233\t2016-06-24 17:16\nCND518\t576\t56.31666666666667\t61.583333333333336\t634.8466\t636.6048\t635.05304\t38000\t467\t309\t8\t5\t49.2925\t12.4462\t2016-06-24 17:27\nCND518\t576\t84.85\t90.11666666666666\t1022.8792\t1024.6268\t1023.1202\t38000\t449\t335\t8\t5\t46.6224\t15.9324\t2016-06-24 16:58\nCND518\t576\t107.25\t112.51666666666667\t1335.2264\t1336.9584\t1335.4915\t38000\t475\t336\t8\t5\t44.1803\t18.1191\t2016-06-24 16:36\nCND518\t576\t45.666666666666664\t50.93333333333333\t484.06577\t485.82462\t484.2608\t38000\t459\t308\t8\t5\t50.1534\t10.8103\t2016-06-24 17:38\nCND518\t576\t122.0\t127.26666666666667\t1547.8892\t1549.6113\t1548.1653\t33675\t486\t327\t8\t5\t42.4946\t19.4692\t2016-06-24 16:21\nCND518\t576\t83.78333333333333\t89.05\t1009.4142\t1011.16296\t1009.65283\t38000\t449\t335\t8\t5\t46.7434\t15.8536\t2016-06-24 17:00\nCND518\t576\t0.0\t3.1333333333333333\t0.6124624\t1.4241931\t0.21760741\t0\t28\t137\t8\t5\t52.3142\t4.7591\t2016-06-24 18:25\nCND518\t576\t135.0\t140.26666666666668\t1715.0939\t1716.8145\t1715.3716\t12775\t369\t341\t8\t5\t41.2954\t20.6931\t2016-06-24 16:08\nCND518\t576\t12.816666666666666\t18.083333333333332\t56.41842\t57.59195\t56.23147\t7725\t320\t276\t8\t5\t52.4931\t5.5368\t2016-06-24 18:10\nCND518\t576\t73.11666666666666\t78.38333333333334\t873.84033\t875.59717\t874.0551\t38000\t456\t309\t8\t5\t47.9076\t14.9433\t2016-06-24 17:10\nCND518\t576\t127.51666666666667\t132.78333333333333\t1628.1349\t1629.8556\t1628.4125\t27250\t454\t302\t8\t5\t41.9039\t20.0351\t2016-06-24 16:16\nCND518\t576\t0.0\t5.266666666666667\t1.4400705\t0.3988846\t1.3844684\t0\t19\t2\t8\t5\t52.313\t4.7413\t2016-06-24 18:23\nCND518\t576\t79.48333333333333\t84.75\t955.2299\t956.983\t955.4581\t38000\t451\t336\t8\t5\t47.2366\t15.5349\t2016-06-24 17:04\nCND518\t576\t25.45\t30.716666666666665\t203.24342\t204.9507\t203.33119\t34375\t483\t311\t8\t5\t51.8541\t7.6443\t2016-06-24 17:58\nCND518\t576\t38.11666666666667\t43.38333333333333\t377.74374\t379.5017\t377.92514\t38000\t466\t307\t8\t5\t50.7481\t9.6171\t2016-06-24 17:45\nCND518\t576\t137.2\t142.46666666666667\t1735.539\t1737.2582\t1735.818\t8800\t260\t1\t8\t5\t41.1164\t20.7878\t2016-06-24 16:06\nCND518\t576\t16.983333333333334\t22.25\t97.78816\t99.28445\t97.73142\t16750\t400\t290\t8\t5\t52.3731\t6.1979\t2016-06-24 18:06\nCND518\t576\t37.016666666666666\t42.28333333333333\t361.9703\t363.72787\t362.1489\t38000\t466\t309\t8\t5\t50.8361\t9.4375\t2016-06-24 17:46\nCND518\t576\t112.46666666666667\t117.73333333333333\t1408.4818\t1410.209\t1408.7526\t38000\t479\t327\t8\t5\t43.5672\t18.5422\t2016-06-24 16:31\nCND518\t576\t76.33333333333333\t81.6\t916.7859\t918.5415\t917.00574\t38000\t451\t334\t8\t5\t47.5898\t15.3024\t2016-06-24 17:07\nCND518\t576\t15.983333333333333\t21.25\t86.92524\t88.36891\t86.84315\t14275\t384\t290\t8\t5\t52.4101\t6.0315\t2016-06-24 18:07\nCND518\t576\t8.516666666666667\t13.783333333333333\t29.706842\t29.96228\t29.312449\t4025\t257\t279\t8\t5\t52.5305\t5.0065\t2016-06-24 18:15\nCND518\t576\t18.066666666666666\t23.333333333333332\t111.00656\t112.547714\t110.97347\t19750\t418\t290\t8\t5\t52.329\t6.3953\t2016-06-24 18:05\nCND518\t576\t115.61666666666666\t120.88333333333334\t1454.755\t1456.4805\t1455.0276\t38000\t482\t327\t8\t5\t43.2118\t18.8538\t2016-06-24 16:28\nCND518\t576\t69.0\t74.26666666666667\t815.6173\t817.3744\t815.831\t38000\t458\t309\t8\t5\t48.2432\t14.3403\t2016-06-24 17:14\nCND518\t576\t90.25\t95.51666666666667\t1093.0894\t1094.8308\t1093.3414\t38000\t466\t329\t8\t5\t45.9966\t16.3366\t2016-06-24 16:53\nCND518\t576\t24.383333333333333\t29.65\t189.10985\t190.8005\t189.18083\t33350\t486\t311\t8\t5\t51.9487\t7.4709\t2016-06-24 17:59\nCND518\t576\t9.616666666666667\t14.883333333333333\t34.63388\t35.265903\t34.304832\t4850\t260\t276\t8\t5\t52.5211\t5.1356\t2016-06-24 18:14\nCND518\t576\t111.45\t116.71666666666667\t1393.9968\t1395.7246\t1394.267\t38000\t477\t327\t8\t5\t43.6783\t18.4438\t2016-06-24 16:32\nCND518\t576\t88.15\t93.41666666666667\t1065.3054\t1067.0493\t1065.5533\t38000\t462\t335\t8\t5\t46.2426\t16.177\t2016-06-24 16:55\nCND518\t576\t72.15\t77.41666666666667\t859.98773\t861.74457\t860.2023\t38000\t456\t309\t8\t5\t47.9876\t14.8004\t2016-06-24 17:11\nCND518\t576\t136.08333333333334\t141.35\t1726.118\t1727.8378\t1726.3967\t10850\t314\t343\t8\t5\t41.1941\t20.7364\t2016-06-24 16:07\nCND518\t576\t131.8\t137.06666666666666\t1681.4008\t1683.1241\t1681.6755\t21450\t370\t341\t8\t5\t41.6046\t20.5582\t2016-06-24 16:11\nCND518\t576\t54.18333333333333\t59.45\t604.3921\t606.1505\t604.5967\t38000\t464\t309\t8\t5\t49.4678\t12.1205\t2016-06-24 17:29\nCND518\t576\t1.0833333333333333\t6.35\t1.8654227\t0.26808608\t1.662129\t0\t105\t182\t8\t5\t52.3185\t4.7389\t2016-06-24 18:22\nCND518\t576\t118.73333333333333\t124.0\t1499.9832\t1501.707\t1500.2576\t36650\t483\t327\t8\t5\t42.8639\t19.1547\t2016-06-24 16:25\nCND518\t576\t86.0\t91.26666666666667\t1037.4746\t1039.221\t1037.7181\t38000\t453\t335\t8\t5\t46.4913\t16.0169\t2016-06-24 16:57\nCND518\t576\t98.78333333333333\t104.05\t1214.3676\t1216.1052\t1214.6251\t38000\t468\t322\t8\t5\t45.122\t17.2926\t2016-06-24 16:45\nCND518\t576\t0.0\t0.0\t0.5628664\t1.4201914\t0.20259362\t0\t10\t137\t8\t5\t52.3137\t4.7589\t2016-06-24 18:29\nCND518\t576\t4.25\t9.516666666666667\t16.019865\t15.226078\t15.557887\t2200\t177\t183\t8\t5\t52.4529\t4.7513\t2016-06-24 18:19\nCND518\t576\t97.73333333333333\t103.0\t1199.4536\t1201.1917\t1199.7106\t38000\t465\t322\t8\t5\t45.2296\t17.1763\t2016-06-24 16:46\nCND518\t576\t62.68333333333333\t67.95\t726.56836\t728.32587\t726.77924\t38000\t466\t310\t8\t5\t48.7589\t13.412\t2016-06-24 17:21\nCND518\t576\t123.15\t128.41666666666666\t1565.0704\t1566.7921\t1565.3472\t32500\t486\t327\t8\t5\t42.3634\t19.5833\t2016-06-24 16:20\nCND518\t576\t92.41666666666667\t97.68333333333334\t1123.717\t1125.4574\t1123.9707\t38000\t466\t322\t8\t5\t45.7746\t16.5786\t2016-06-24 16:51\nCND518\t576\t109.35\t114.61666666666666\t1364.2985\t1366.0281\t1364.5663\t38000\t472\t336\t8\t5\t43.9261\t18.2718\t2016-06-24 16:34\nCND518\t576\t59.483333333333334\t64.75\t680.442\t682.1999\t680.6508\t38000\t469\t309\t8\t5\t49.0281\t12.9289\t2016-06-24 17:24\nCND518\t576\t44.516666666666666\t49.78333333333333\t467.75412\t469.51297\t467.94748\t38000\t461\t308\t8\t5\t50.2453\t10.6295\t2016-06-24 17:39\nCND518\t576\t99.88333333333334\t105.15\t1230.6064\t1232.3436\t1230.8646\t38000\t471\t322\t8\t5\t45.0048\t17.4188\t2016-06-24 16:43\nCND518\t576\t64.8\t70.06666666666666\t756.194\t757.9513\t756.4062\t38000\t463\t311\t8\t5\t48.5845\t13.7189\t2016-06-24 17:18\nCND518\t576\t2.1166666666666667\t7.383333333333334\t5.6313562\t4.685266\t5.186343\t400\t151\t183\t8\t5\t52.3582\t4.7424\t2016-06-24 18:21\nCND518\t576\t34.85\t40.11666666666667\t331.78006\t333.53558\t331.94867\t38000\t470\t312\t8\t5\t51.0273\t9.1104\t2016-06-24 17:48\nCND518\t576\t124.2\t129.46666666666667\t1580.5538\t1582.275\t1580.8309\t31500\t479\t327\t8\t5\t42.2454\t19.6862\t2016-06-24 16:19\nCND518\t576\t3.2\t8.466666666666667\t10.661752\t9.82789\t10.20247\t1275\t158\t183\t8\t5\t52.4044\t4.7468\t2016-06-24 18:20\nCND518\t576\t119.78333333333333\t125.05\t1515.5548\t1517.2782\t1515.8298\t35700\t486\t327\t8\t5\t42.7442\t19.2578\t2016-06-24 16:24\nCND518\t576\t57.416666666666664\t62.68333333333333\t650.5623\t652.3204\t650.7696\t38000\t468\t309\t8\t5\t49.2015\t12.6131\t2016-06-24 17:26\nCND518\t576\t39.2\t44.46666666666667\t392.77914\t394.53738\t392.96292\t38000\t466\t307\t8\t5\t50.6647\t9.7881\t2016-06-24 17:44\nCND518\t576\t95.61666666666666\t100.88333333333334\t1169.708\t1171.4469\t1169.9637\t38000\t465\t322\t8\t5\t45.4438\t16.9428\t2016-06-24 16:48\nCND518\t576\t103.08333333333333\t108.35\t1276.9783\t1278.7142\t1277.2383\t38000\t479\t322\t8\t5\t44.6696\t17.7764\t2016-06-24 16:40\nCND518\t576\t23.333333333333332\t28.6\t175.56975\t177.23871\t175.62146\t31700\t471\t311\t8\t5\t52.0415\t7.3007\t2016-06-24 18:00\nCND518\t576\t130.73333333333332\t136.0\t1668.918\t1670.6416\t1669.1926\t22450\t413\t301\t8\t5\t41.6973\t20.4723\t2016-06-24 16:13\nCND518\t576\t22.283333333333335\t27.55\t162.85246\t164.49382\t162.8826\t29350\t455\t311\t8\t5\t52.1304\t7.1358\t2016-06-24 18:01\nCND518\t576\t110.4\t115.66666666666667\t1379.117\t1380.8453\t1379.3862\t38000\t473\t336\t8\t5\t43.7969\t18.3493\t2016-06-24 16:33\nCND518\t576\t71.05\t76.31666666666666\t844.5996\t846.35657\t844.81384\t38000\t456\t309\t8\t5\t48.0764\t14.6413\t2016-06-24 17:12\nCND518\t576\t48.86666666666667\t54.13333333333333\t529.22974\t530.9885\t529.4289\t38000\t460\t308\t8\t5\t49.8977\t11.3069\t2016-06-24 17:34\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152840146_-1629339651",
      "id": "20180128-152040_1744302051",
      "dateCreated": "Jan 28, 2018 3:20:40 PM",
      "dateStarted": "Jan 28, 2018 4:10:44 PM",
      "dateFinished": "Jan 28, 2018 4:15:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n\r\n```\r\n1) flight \u003d Flight number like KL1234\r\n2) sum_hour_touchdown \u003d The sum of planes landing in the hour this flight landed (eg landing time 16:34, will show 500 planes landing in hour 16)\r\n3) time_till_landing_minutes \u003d number of minutes in the future till this flight landed\r\n4) time_till_onblock_minutes \u003d number of minutes in the future till this flight parked at the pier (gate)\r\n5) distance_to_ams \u003d distance in (miles) to Schiphol.\r\n6) distance_to_runway \u003d distance in miles to landing runway\r\n7) distance_to_pier \u003d distance in miles to on-block pier\r\n8) altitude \u003d altitude in feet\r\n9) speed \u003d speed in miles per hour\r\n10) heading \u003d direction in degrees (0-359)\r\n11) runway_touchdown \u003d Runway used to land (code 1-12). Code 1-6 are the same runways as 7-12. However the heading used while landing determines the code used (eg code 1 means polderbaan landing from north, 7 from south (which is never used).\r\n12) pier_onblock \u003d Code of pier used to park . Code is based on pier A-H. Not complete yet. Might better to replace with VOP\r\n```\r\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 3:21:17 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cpre\u003e\u003ccode\u003e1) flight \u003d Flight number like KL1234\r\n2) sum_hour_touchdown \u003d The sum of planes landing in the hour this flight landed (eg landing time 16:34, will show 500 planes landing in hour 16)\r\n3) time_till_landing_minutes \u003d number of minutes in the future till this flight landed\r\n4) time_till_onblock_minutes \u003d number of minutes in the future till this flight parked at the pier (gate)\r\n5) distance_to_ams \u003d distance in (miles) to Schiphol.\r\n6) distance_to_runway \u003d distance in miles to landing runway\r\n7) distance_to_pier \u003d distance in miles to on-block pier\r\n8) altitude \u003d altitude in feet\r\n9) speed \u003d speed in miles per hour\r\n10) heading \u003d direction in degrees (0-359)\r\n11) runway_touchdown \u003d Runway used to land (code 1-12). Code 1-6 are the same runways as 7-12. However the heading used while landing determines the code used (eg code 1 means polderbaan landing from north, 7 from south (which is never used).\r\n12) pier_onblock \u003d Code of pier used to park . Code is based on pier A-H. Not complete yet. Might better to replace with VOP\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152856087_531825569",
      "id": "20180128-152056_1609989461",
      "dateCreated": "Jan 28, 2018 3:20:56 PM",
      "dateStarted": "Jan 28, 2018 3:21:17 PM",
      "dateFinished": "Jan 28, 2018 3:21:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Optional save and load the intermediate result",
      "text": "%pyspark\n\n#PYTHON\n#ml_df.write.parquet(\"/tmp/somedir/ml_df.parquet\")\nml_df \u003d spark.read.parquet(\"/tmp/somedir/ml_df.parquet\")",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:36:03 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1517156210075_2136965090",
      "id": "20180128-161650_1037699125",
      "dateCreated": "Jan 28, 2018 4:16:50 PM",
      "dateStarted": "Feb 4, 2018 12:36:03 PM",
      "dateFinished": "Feb 4, 2018 12:36:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### If you plan to use this model for realtime predictions you have to load this model in a scala variable right now, as Pyspark misses the MlWriter class it seems",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 6:41:51 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eIf you plan to use this model for realtime predictions you have to load this model in a scala variable right now, as Pyspark misses the MlWriter class it seems\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517164855226_1869205202",
      "id": "20180128-184055_1473349605",
      "dateCreated": "Jan 28, 2018 6:40:55 PM",
      "dateStarted": "Jan 28, 2018 6:41:51 PM",
      "dateFinished": "Jan 28, 2018 6:41:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nval mlDf \u003d spark.read.parquet(\"/tmp/somedir/ml_df.parquet\")\n.registerTempTable(\"flightmunged\")\n\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:37:11 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "warning: there was one deprecation warning; re-run with -deprecation for details\nmlDf: Unit \u003d ()\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517164832321_1349778863",
      "id": "20180128-184032_757124968",
      "dateCreated": "Jan 28, 2018 6:40:32 PM",
      "dateStarted": "Feb 4, 2018 12:37:11 PM",
      "dateFinished": "Feb 4, 2018 12:37:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Train model  and create predictions for test set",
      "text": "%pyspark\n\n#ml_df is our source\n\n#You might also just open the saved version in the section below has it takes a while to execute (9 minutes on my laptop)\n#If you saved the model earlier, it will just take 30 seconds to train and predict\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n#CREATE AND TRAIN MODEL\ntraining_data, testing_data \u003d ml_df.randomSplit([0.7, 0.3])\nassembler \u003d VectorAssembler(inputCols\u003d[\"heading\",\"sum_hour_touchdown\", \"distance_to_runway\",\"altitude\",\"speed\",\"runway_touchdown\"],outputCol\u003d\"features\")\nRegressor \u003d DecisionTreeRegressor(featuresCol\u003d\"features\",labelCol\u003d\"time_till_landing_minutes\",maxDepth\u003d25)\npipeline \u003d Pipeline(stages\u003d[assembler,Regressor])\nmodel \u003d pipeline.fit(training_data)\n\n#SAVE MODEL\n#model.write.save(\"/tmp/somedir/flight-pipeline\")\n\n#PREDICT TEST SET\nmodel.transform(ml_df).registerTempTable(\"flightpredictions\")  #.cache\n\n#Alternative test first using:\n#predictions \u003d model.transform(testing_data)\n#modelEvaluator \u003d RegressionEvaluator(labelCol\u003d\"time_till_landing_minutes\")\n#modelError \u003d modelEvaluator.evaluate(predictions,{modelEvaluator.metricName: \"mae\"})\n#modelError #3 half minutes\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:37:44 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1517152877557_12045129",
      "id": "20180128-152117_812737674",
      "dateCreated": "Jan 28, 2018 3:21:17 PM",
      "dateStarted": "Feb 4, 2018 12:37:44 PM",
      "dateFinished": "Feb 4, 2018 12:38:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### The same code but this time in Scala and including a save model command, which is not in pyspark. Required for online predictions.",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 6:42:48 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eThe same code but this time in Scala and including a save model command, which is not in pyspark. Required for online predictions.\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517164932862_1899923817",
      "id": "20180128-184212_1493552976",
      "dateCreated": "Jan 28, 2018 6:42:12 PM",
      "dateStarted": "Jan 28, 2018 6:42:48 PM",
      "dateFinished": "Jan 28, 2018 6:42:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\n//Read in the saved ml_df using some sections back in scala variable.\n\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.Pipeline\n\nval Array(trainingData, testData) \u003d mlDf.randomSplit(Array(0.7, 0.3))\nval assembler \u003d new VectorAssembler()\n  .setInputCols(Array(\"heading\", \"sum_hour_touchdown\", \"distance_to_runway\",\"altitude\",\"speed\",\"runway_touchdown\"))\n  .setOutputCol(\"features\")\nval regressor \u003d new DecisionTreeRegressor().setLabelCol(\"time_till_landing_minutes\").setFeaturesCol(\"features\").setMaxDepth(25)\nval pipeline \u003d new Pipeline().setStages(Array(assembler, regressor))\nval model \u003d pipeline.fit(trainingData)\nmodel.write.overwrite().save(\"/tmp/somedir/flight-pipeline\")\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 6:43:33 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.DecisionTreeRegressor\nimport org.apache.spark.ml.Pipeline\nmlDf: org.apache.spark.sql.DataFrame \u003d [flight: string, sum_hour_touchdown: bigint ... 13 more fields]\ntrainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [flight: string, sum_hour_touchdown: bigint ... 13 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [flight: string, sum_hour_touchdown: bigint ... 13 more fields]\nassembler: org.apache.spark.ml.feature.VectorAssembler \u003d vecAssembler_25795033c90f\nregressor: org.apache.spark.ml.regression.DecisionTreeRegressor \u003d dtr_a69db3916ca7\npipeline: org.apache.spark.ml.Pipeline \u003d pipeline_26b15f3ba2db\nmodel: org.apache.spark.ml.PipelineModel \u003d pipeline_26b15f3ba2db\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517163952917_-459226743",
      "id": "20180128-182552_2100671219",
      "dateCreated": "Jan 28, 2018 6:25:52 PM",
      "dateStarted": "Jan 28, 2018 6:39:15 PM",
      "dateFinished": "Jan 28, 2018 6:39:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Lets create an animation on our world map by creating a stream out of our predictions set",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 6:44:36 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eLets create an animation on our world map by creating a stream out of our predictions set\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517165024358_446432773",
      "id": "20180128-184344_669832353",
      "dateCreated": "Jan 28, 2018 6:43:44 PM",
      "dateStarted": "Jan 28, 2018 6:44:36 PM",
      "dateFinished": "Jan 28, 2018 6:44:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Read predictions and some metadata in seperate dataframe",
      "text": "%spark\n\nval flightMapDf \u003d spark.sql(\"select flight,minute,lat,lon,time_till_landing_minutes,prediction from flightpredictions\").sort(col(\"minute\").asc)\nval flightArray \u003d flightMapDf.collect  //Lets create an array so we can iterate a custom spark receiver",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:38:23 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "flightMapDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [flight: string, minute: string ... 4 more fields]\nflightArray: Array[org.apache.spark.sql.Row] \u003d Array([DL160,2016-06-23 23:31,54.016,-57.1534,266.46666666666664,265.46666666666664], [KL706,2016-06-23 23:31,-22.7995,-43.2324,656.4833333333333,656.4833333333333], [KL792,2016-06-23 23:31,-19.7594,-43.9315,636.9833333333333,636.9833333333333], [DL56,2016-06-23 23:31,40.7745,-111.9742,581.1666666666666,581.1666666666666], [KL808,2016-06-23 23:31,52.1665,66.7056,312.76666666666665,302.63846153846157], [DL72,2016-06-23 23:31,36.1432,-77.2882,415.31666666666666,429.25], [MP6332,2016-06-23 23:31,40.1743,-72.0937,366.6666666666667,357.25625], [CX271,2016-06-23 23:31,52.0412,94.1936,414.3,414.29999999999995], [HV6120,2016-06-23 23:31,52.3042,4.7704,1333.0833333333333,0.0], [OR338,2016-06-23 23:31,39.9246,-73.3105,364.55,364.55], [UA909,2016-06-2..."
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152894978_194613677",
      "id": "20180128-152134_1492778351",
      "dateCreated": "Jan 28, 2018 3:21:34 PM",
      "dateStarted": "Feb 4, 2018 12:38:23 PM",
      "dateFinished": "Feb 4, 2018 12:38:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Custom spark streaming receiver to read our predictions",
      "text": "%spark\r\n\r\nimport org.apache.spark.streaming.receiver.Receiver\r\nimport org.apache.spark.internal.Logging\r\nimport org.apache.spark.storage.StorageLevel\r\nimport scala.util.Random\r\n\r\nclass FlightReplyData(timeBreakSec : Double) extends org.apache.spark.streaming.receiver.Receiver[String](org.apache.spark.storage.StorageLevel.MEMORY_AND_DISK_2) { \r\n\r\n  def onStart() {\r\n    // Start the thread that receives data over a connection\r\n    new Thread(\"Socket Receiver Flight\") {\r\n      override def run() { receive() }\r\n    }.start()\r\n  }\r\n\r\n  def onStop() {\r\n   // There is nothing much to do as the thread calling receive()\r\n   // is designed to stop by itself isStopped() returns false\r\n  }\r\n\r\n  /** Create a socket connection and receive data until receiver is stopped */\r\n  private def receive() {\r\n   var lastTime \u003d \"\"\r\n   var resultArray \u003d List[String]()\r\n   while(!isStopped()) {     \r\n        //Lets create an array to return as stream per minute (as our set is ordered by minute (event time)\r\n        flightArray.foreach(r \u003d\u003e {\r\n            val result : Double \u003d r(5).asInstanceOf[Double] - r(4).asInstanceOf[Double]\r\n            \r\n            resultArray \u003d  (r(0) + \",\" + r(1) + \",\" + r(2) + \",\" + r(3) + \",\"  + \" (\" + result.toInt +\")\" ).toString :: resultArray\r\n            \r\n            if (r(1).toString !\u003d lastTime) {\r\n                lastTime \u003d r(1).toString\r\n                store(resultArray.reverse.iterator) //reverse to start with oldest time\r\n                resultArray \u003d List[String]()\r\n                Thread.sleep((timeBreakSec * 1000).toLong) //break to give stream some air\r\n            }\r\n            \r\n        }) \r\n       \r\n    }\r\n  }\r\n}\r\n\r\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:38:41 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.streaming.receiver.Receiver\nimport org.apache.spark.internal.Logging\nimport org.apache.spark.storage.StorageLevel\nimport scala.util.Random\ndefined class FlightReplyData\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152907556_-1297442557",
      "id": "20180128-152147_164702341",
      "dateCreated": "Jan 28, 2018 3:21:47 PM",
      "dateStarted": "Feb 4, 2018 12:38:41 PM",
      "dateFinished": "Feb 4, 2018 12:38:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Map showing flights and predictions delta in minutes compared to reality (starts in next section)",
      "text": "%angular\n\n\u003clink rel\u003d\"stylesheet\" href\u003d\"https://unpkg.com/leaflet@1.2.0/dist/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"mapt4\" style\u003d\"height: 800px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027mapt4\u0027).setView([30.00, -30.00], 3);\n    //L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027).addTo(map);\n    //L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003ds\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n    //maxZoom: 20 ,subdomains:[\u0027mt0\u0027]\n    //}).addTo(map);\n    L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003dp\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n        maxZoom: 20,\n        subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]\n    }).addTo(map);\n\n                \n    //subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]            \n    var geoMarkers \u003d L.layerGroup().addTo(map); //Ship icons to be stored in layer, so we can clear all of them each update\n\n    var el \u003d angular.element($(\u0027#mapt4\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027planelocations\u0027, function(newValue, oldValue) {\n            geoMarkers.clearLayers(); //Remove the ship icons at their current location.\n           \n            var isTimeSet \u003d false\n            \n            angular.forEach(newValue, function(event) {\n                if (event) {\n                     if (isTimeSet \u003d\u003d false) {\n                        //lets add time/minute in middle of map\n                            var divIcon \u003d L.divIcon({ \n                            html: \u0027\u003cspan style\u003d\"color:white;font-size: 15pt\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[3] + \u0027\u003c/span\u003e\u0027\n                        });\n                        L.marker(new L.LatLng(28.45900, -45.7509), {icon: divIcon }).addTo(geoMarkers);\n                        isTimeSet \u003d true;\n                    }\n                    \n                    \n                    new L.Marker([event.values[1], event.values[2]], {\n                        icon: new L.DivIcon({\n                        className: \u0027my-div-icon\u0027,\n                        html: \u0027\u003cimg width\u003d\"10px\" height\u003d\"10px\" class\u003d\"my-div-image\" src\u003d\"http://www.planetoftunes.com/dtp/png_test/red_circle.png\"/\u003e\u0027+\n                        \u0027\u003cspan style\u003d\"color:DarkGreen\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[0] + event.values[4] + \u0027\u003c/span\u003e\u0027\n                    })}).addTo(geoMarkers);\n\n                }\n                   \n            });\n            isTimeSet \u003d false;\n            \n        })\n    });\n}\n\nif (window.locationWatcher) { window.locationWatcher(); }\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://unpkg.com/leaflet@1.2.0/dist/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:38:50 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/undefined",
        "title": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "ANGULAR",
            "data": "\u003clink rel\u003d\"stylesheet\" href\u003d\"https://unpkg.com/leaflet@1.2.0/dist/leaflet.css\" /\u003e\n\u003cdiv id\u003d\"mapt4\" style\u003d\"height: 800px; width: 100%\"\u003e\u003c/div\u003e\n\n\u003cscript type\u003d\"text/javascript\"\u003e\nfunction initMap() {\n    var map \u003d L.map(\u0027mapt4\u0027).setView([30.00, -30.00], 3);\n    //L.tileLayer(\u0027http://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\u0027).addTo(map);\n    //L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003ds\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n    //maxZoom: 20 ,subdomains:[\u0027mt0\u0027]\n    //}).addTo(map);\n    L.tileLayer(\u0027http://{s}.google.com/vt/lyrs\u003dp\u0026x\u003d{x}\u0026y\u003d{y}\u0026z\u003d{z}\u0027,{\n        maxZoom: 20,\n        subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]\n    }).addTo(map);\n\n                \n    //subdomains:[\u0027mt0\u0027,\u0027mt1\u0027,\u0027mt2\u0027,\u0027mt3\u0027]            \n    var geoMarkers \u003d L.layerGroup().addTo(map); //Ship icons to be stored in layer, so we can clear all of them each update\n\n    var el \u003d angular.element($(\u0027#mapt4\u0027).parent(\u0027.ng-scope\u0027));\n    angular.element(el).ready(function() {\n        window.locationWatcher \u003d el.scope().compiledScope.$watch(\u0027planelocations\u0027, function(newValue, oldValue) {\n            geoMarkers.clearLayers(); //Remove the ship icons at their current location.\n           \n            var isTimeSet \u003d false\n            \n            angular.forEach(newValue, function(event) {\n                if (event) {\n                     if (isTimeSet \u003d\u003d false) {\n                        //lets add time/minute in middle of map\n                            var divIcon \u003d L.divIcon({ \n                            html: \u0027\u003cspan style\u003d\"color:white;font-size: 15pt\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[3] + \u0027\u003c/span\u003e\u0027\n                        });\n                        L.marker(new L.LatLng(28.45900, -45.7509), {icon: divIcon }).addTo(geoMarkers);\n                        isTimeSet \u003d true;\n                    }\n                    \n                    \n                    new L.Marker([event.values[1], event.values[2]], {\n                        icon: new L.DivIcon({\n                        className: \u0027my-div-icon\u0027,\n                        html: \u0027\u003cimg width\u003d\"10px\" height\u003d\"10px\" class\u003d\"my-div-image\" src\u003d\"http://www.planetoftunes.com/dtp/png_test/red_circle.png\"/\u003e\u0027+\n                        \u0027\u003cspan style\u003d\"color:DarkGreen\" class\u003d\"my-div-span\"\u003e\u0027 + event.values[0] + event.values[4] + \u0027\u003c/span\u003e\u0027\n                    })}).addTo(geoMarkers);\n\n                }\n                   \n            });\n            isTimeSet \u003d false;\n            \n        })\n    });\n}\n\nif (window.locationWatcher) { window.locationWatcher(); }\n\n// ensure we only load the script once, seems to cause issues otherwise\nif (window.L) {\n    initMap();\n} else {\n    console.log(\u0027Loading Leaflet library\u0027);\n    var sc \u003d document.createElement(\u0027script\u0027);\n    sc.type \u003d \u0027text/javascript\u0027;\n    sc.src \u003d \u0027https://unpkg.com/leaflet@1.2.0/dist/leaflet.js\u0027;\n    sc.onload \u003d initMap;\n    sc.onerror \u003d function(err) { alert(err); }\n    document.getElementsByTagName(\u0027head\u0027)[0].appendChild(sc);\n}\n\u003c/script\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152920688_-1096572983",
      "id": "20180128-152200_1222729209",
      "dateCreated": "Jan 28, 2018 3:22:00 PM",
      "dateStarted": "Feb 4, 2018 12:38:50 PM",
      "dateFinished": "Feb 4, 2018 12:38:50 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\n//Structured streaming can be used to read csv but this just reads too quickly.\n//Notice this method below is ideal to convert a pulled Api to a stream in Spark\n\nimport org.apache.spark.streaming.{Milliseconds,Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.expressions.Window\n\nz.angularUnbind(\"planelocations\") //Our geo map in the previous section will be updating  once we update this Zeppelin context variable\n\nStreamingContext.getActive.foreach { _.stop(stopSparkContext \u003d false) }\nval ssc \u003d new StreamingContext(sc, Milliseconds(500)) //Our stream will update each second \nval flightDStream \u003d ssc.receiverStream(new FlightReplyData(0.4)) \n\n//val windowFlightDStream \u003d flightDStream\n\n// Use a Window Dstream with location of couple of runs . Required if you want to aggregate over a longer historical period or prevent duplicates/or dissapearing planes. \nval windowFlightDStream \u003d flightDStream.window(Milliseconds(1000),Milliseconds(500)) \n\n//Get the latest location of each plane (otherwise we would see double, or missings planes on the map). \n//and return it to our Leaflet using a Zepplin Variable named locations (Expecting Array of arrays Name,lat,lon)                                                        \nwindowFlightDStream.foreachRDD { rdd \u003d\u003e  \n\n                        val inputDf \u003d rdd.map (s \u003d\u003e s.split(\",\"))\n                                    .map(arr \u003d\u003e (arr(0),arr(1),arr(2),arr(3),arr(4)) )\n                                    .toDF(\"id\",\"timest\",\"lat\",\"lon\",\"ttl\").na.drop //ttl \u003d Time till landing, pttl also but predicted\n                                    \n                        //option 1 just have a quick but flickering, missing animation\n                        //z.angularBind(\"planelocations\", inputDf.select($\"id\",$\"lat\",$\"lon\",$\"timest\").collect)\n                        \n                        //option 2 group by plane and get the latest location\n                        //val maxTimeStampDf \u003d inputDf.groupBy($\"id\").agg(max($\"timest\") as \"maxtimestamp\").withColumnRenamed(\"id\", \"id2\")\n                        //val latestLocationDf \u003d inputDf.join(maxTimeStampDf,inputDf(\"id\") \u003d\u003d\u003d maxTimeStampDf(\"id2\")).filter($\"timest\" \u003d\u003d\u003d $\"maxtimestamp\")  //.orderBy($\"id\".asc,$\"timestamp\".asc).cache\n                        //z.angularBind(\"planelocations\", latestLocationDf.select($\"id\",$\"lat\",$\"lon\",$\"timest\").collect)\n                        \n                        //Option 3 get a minute field from 1 row and filter all rows with it (and make sure the data is in a bit quicker then the stream interval)\n                        val window \u003d Window.partitionBy($\"id\").orderBy($\"timest\".asc)\n                        z.angularBind(\"planelocations\", inputDf.withColumn(\"rank\", row_number().over(window)).where($\"rank\" \u003d\u003d\u003d 1).select($\"id\",$\"lat\",$\"lon\",$\"timest\",$\"ttl\").collect  ) //.where($\"rank\" \u003d\u003d\u003d 1)\n                        \n}                       \n                                                        \nssc.start()\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:38:58 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.streaming.{Milliseconds, Seconds, StreamingContext}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.expressions.Window\nssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@50a7e344\nflightDStream: org.apache.spark.streaming.dstream.ReceiverInputDStream[String] \u003d org.apache.spark.streaming.dstream.PluggableInputDStream@2fbf8c19\nwindowFlightDStream: org.apache.spark.streaming.dstream.DStream[String] \u003d org.apache.spark.streaming.dstream.WindowedDStream@7b6ad402\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517152936459_1414713736",
      "id": "20180128-152216_2052035651",
      "dateCreated": "Jan 28, 2018 3:22:16 PM",
      "dateStarted": "Feb 4, 2018 12:38:58 PM",
      "dateFinished": "Feb 4, 2018 12:39:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### Lets create some realtime predictions . It requires the model to be saved using MlWriter\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 6:46:15 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eLets create some realtime predictions . It requires the model to be saved using MlWriter\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517165151209_-1897256667",
      "id": "20180128-184551_1727382354",
      "dateCreated": "Jan 28, 2018 6:45:51 PM",
      "dateStarted": "Jan 28, 2018 6:46:15 PM",
      "dateFinished": "Jan 28, 2018 6:46:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "check sample input row ",
      "text": "%sql\n\nselect heading, sum_hour_touchdown, distance_to_runway,altitude,speed,runway_touchdown from flightmunged limit 1\n",
      "user": "anonymous",
      "dateUpdated": "Jan 28, 2018 7:15:10 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sql",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "heading\tsum_hour_touchdown\tdistance_to_runway\taltitude\tspeed\trunway_touchdown\n304\t405\t1247.3434\t17625\t390\t1\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517166552804_-579145283",
      "id": "20180128-190912_1170758466",
      "dateCreated": "Jan 28, 2018 7:09:12 PM",
      "dateStarted": "Jan 28, 2018 7:11:29 PM",
      "dateFinished": "Jan 28, 2018 7:11:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Test by throwing this input to our model",
      "text": "%spark\n\nimport org.apache.spark.ml._\nval pipelineModel \u003d PipelineModel.read.load(\"/tmp/somedir/flight-pipeline\")\n\n//Test by throwing some input to the model. Right column shows prediction\u003d\u003d\nz.show(pipelineModel.transform(spark.sql(\"select heading, sum_hour_touchdown, distance_to_runway,altitude,speed,runway_touchdown from flightmunged limit 1 \")).select(\"prediction\").toDF  )   //.registerTempTable(\"flightpredictionsrealtime\")",
      "user": "anonymous",
      "dateUpdated": "Jan 30, 2018 1:14:36 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.ml._\npipelineModel: org.apache.spark.ml.PipelineModel \u003d pipeline_26b15f3ba2db\norg.apache.spark.sql.AnalysisException: Table or view not found: flightmunged; line 1 pos 92\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:643)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:595)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:625)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:618)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:62)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:61)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:59)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:618)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:564)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:69)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:67)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n  ... 132 elided\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517165178681_1907571178",
      "id": "20180128-184618_1046062024",
      "dateCreated": "Jan 28, 2018 6:46:18 PM",
      "dateStarted": "Jan 28, 2018 8:19:18 PM",
      "dateFinished": "Jan 28, 2018 8:19:23 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Lets stream our input and create online predictions using structured streaming",
      "text": "%spark\n\nimport org.apache.spark.sql.types._ \nimport org.apache.spark.sql.functions._ \nimport scala.concurrent.duration._ \nimport org.apache.spark.sql.streaming.ProcessingTime \nimport org.apache.spark.sql.streaming.OutputMode.Complete \nimport org.apache.spark.ml._\n\nval pipelineModel \u003d PipelineModel.read.load(\"/tmp/somedir/flight-pipeline\")\nval mlDf \u003d spark.read.parquet(\"/tmp/somedir/ml_df.parquet\")\n\n//INPUT\nval streamingInputDF \u003d pipelineModel.transform(spark.readStream\n                            .format(\"parquet\")\n                            .schema(mlDf.schema)\n                            .option(\"maxFilesPerTrigger\", 1)\n                            .load(\"/tmp/somedir/ml_df.parquet\")\n                            .select(\"flight\",\"minute\", \"heading\", \"sum_hour_touchdown\", \"distance_to_runway\",\"altitude\",\"speed\",\"runway_touchdown\")  )\n\n//OUTPUT THIS WORKS BUT NOT FOR THI TEST, HANDY FOR GROUPING\n//val streamingCountsDF \u003d streamingInputDF.groupBy($\"flight\").count()   //PEr 2 seconds print number a flight was seen\n//val query \u003d streamingCountsDF.writeStream.format(\"console\").trigger(ProcessingTime(2.seconds)).outputMode(Complete).queryName(\"counts\").start().awaitTermination() \n\n//OUTPUT (WORKS)\nval query \u003d streamingInputDF.select(\"flight\",\"minute\",\"prediction\")\n                            .writeStream.format(\"memory\")   //or console\n                            .trigger(ProcessingTime(2.seconds))\n                            .queryName(\"minutesland\")\n                            .start() //.awaitTermination() \n//to kill run bash on Docker and kill spark. Timeout does not work within zeppelin icm structured streaming (it does with dstreams)\n//If you use memory as output, just use sql to query output, and query.stop to stop stream",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:27:43 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport scala.concurrent.duration._\nimport org.apache.spark.sql.streaming.ProcessingTime\nimport org.apache.spark.sql.streaming.OutputMode.Complete\nimport org.apache.spark.ml._\npipelineModel: org.apache.spark.ml.PipelineModel \u003d pipeline_26b15f3ba2db\nmlDf: org.apache.spark.sql.DataFrame \u003d [flight: string, sum_hour_touchdown: bigint ... 13 more fields]\nstreamingInputDF: org.apache.spark.sql.DataFrame \u003d [flight: string, minute: string ... 8 more fields]\nwarning: there was one deprecation warning; re-run with -deprecation for details\nquery: org.apache.spark.sql.streaming.StreamingQuery \u003d org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@106b97a6\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517163162151_-1438028698",
      "id": "20180128-181242_1333594297",
      "dateCreated": "Jan 28, 2018 6:12:42 PM",
      "dateStarted": "Feb 4, 2018 12:27:43 PM",
      "dateFinished": "Feb 4, 2018 12:27:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect * from minutesland where flight\u003d\"KL888\" order by minute asc\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:28:03 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "flight\tminute\tprediction\nKL888\t2016-06-24 04:29\t699.1541666666667\nKL888\t2016-06-24 04:30\t710.3666666666667\nKL888\t2016-06-24 04:31\t699.7583333333332\nKL888\t2016-06-24 04:32\t699.6777777777776\nKL888\t2016-06-24 04:33\t699.1541666666667\nKL888\t2016-06-24 04:34\t701.4888888888887\nKL888\t2016-06-24 04:35\t701.4888888888887\nKL888\t2016-06-24 04:36\t701.4888888888887\nKL888\t2016-06-24 04:37\t702.833333333333\nKL888\t2016-06-24 04:38\t696.2699999999999\nKL888\t2016-06-24 04:39\t699.6777777777776\nKL888\t2016-06-24 04:40\t696.2699999999999\nKL888\t2016-06-24 04:42\t696.2699999999999\nKL888\t2016-06-24 04:43\t702.833333333333\nKL888\t2016-06-24 04:44\t696.2699999999999\nKL888\t2016-06-24 04:45\t699.1541666666667\nKL888\t2016-06-24 04:45\t699.6777777777776\nKL888\t2016-06-24 04:47\t701.4888888888887\nKL888\t2016-06-24 04:48\t692.3166666666667\nKL888\t2016-06-24 04:49\t699.1541666666667\nKL888\t2016-06-24 04:50\t699.6777777777776\nKL888\t2016-06-24 04:51\t692.3166666666667\nKL888\t2016-06-24 04:52\t699.6777777777776\nKL888\t2016-06-24 04:53\t699.1541666666667\nKL888\t2016-06-24 04:54\t699.6777777777776\nKL888\t2016-06-24 04:56\t696.2699999999999\nKL888\t2016-06-24 04:57\t683.5833333333334\nKL888\t2016-06-24 04:58\t645.6333333333333\nKL888\t2016-06-24 04:59\t681.4666666666667\nKL888\t2016-06-24 05:00\t680.35\nKL888\t2016-06-24 05:01\t679.3333333333334\nKL888\t2016-06-24 05:02\t678.3166666666667\nKL888\t2016-06-24 05:03\t677.25\nKL888\t2016-06-24 05:04\t676.1666666666666\nKL888\t2016-06-24 05:05\t675.2\nKL888\t2016-06-24 05:06\t674.1\nKL888\t2016-06-24 05:07\t673.15\nKL888\t2016-06-24 05:08\t672.1\nKL888\t2016-06-24 05:09\t670.9666666666667\nKL888\t2016-06-24 05:10\t670.0\nKL888\t2016-06-24 05:11\t613.7277777777778\nKL888\t2016-06-24 05:12\t663.7\nKL888\t2016-06-24 05:13\t663.7\nKL888\t2016-06-24 05:14\t665.8\nKL888\t2016-06-24 05:15\t663.7\nKL888\t2016-06-24 05:16\t663.7\nKL888\t2016-06-24 05:18\t624.5166666666664\nKL888\t2016-06-24 05:18\t663.7\nKL888\t2016-06-24 05:20\t657.0916666666667\nKL888\t2016-06-24 05:21\t657.0916666666667\nKL888\t2016-06-24 05:22\t657.0916666666667\nKL888\t2016-06-24 05:23\t657.0916666666667\nKL888\t2016-06-24 05:24\t657.0916666666667\nKL888\t2016-06-24 05:25\t657.0916666666667\nKL888\t2016-06-24 05:26\t586.2166666666666\nKL888\t2016-06-24 05:27\t653.2\nKL888\t2016-06-24 05:28\t650.55\nKL888\t2016-06-24 05:29\t650.55\nKL888\t2016-06-24 05:30\t650.55\nKL888\t2016-06-24 05:31\t635.8777777777777\nKL888\t2016-06-24 05:32\t632.4833333333333\nKL888\t2016-06-24 05:33\t632.4833333333333\nKL888\t2016-06-24 05:34\t645.7\nKL888\t2016-06-24 05:35\t639.0595238095237\nKL888\t2016-06-24 05:37\t639.0595238095237\nKL888\t2016-06-24 05:37\t639.0595238095237\nKL888\t2016-06-24 05:39\t639.0595238095237\nKL888\t2016-06-24 05:40\t639.0595238095237\nKL888\t2016-06-24 05:41\t639.0595238095237\nKL888\t2016-06-24 05:42\t639.0595238095237\nKL888\t2016-06-24 05:43\t639.0595238095237\nKL888\t2016-06-24 05:44\t639.0595238095237\nKL888\t2016-06-24 05:45\t632.4833333333333\nKL888\t2016-06-24 05:46\t632.8999999999996\nKL888\t2016-06-24 05:47\t632.8999999999996\nKL888\t2016-06-24 05:48\t632.8999999999996\nKL888\t2016-06-24 05:49\t639.0595238095237\nKL888\t2016-06-24 05:50\t635.8777777777777\nKL888\t2016-06-24 05:51\t635.8777777777777\nKL888\t2016-06-24 05:53\t632.4833333333333\nKL888\t2016-06-24 05:54\t632.4833333333333\nKL888\t2016-06-24 05:55\t632.4833333333333\nKL888\t2016-06-24 05:56\t620.9\nKL888\t2016-06-24 05:57\t620.9\nKL888\t2016-06-24 05:58\t620.9\nKL888\t2016-06-24 05:59\t620.9\nKL888\t2016-06-24 06:00\t620.9\nKL888\t2016-06-24 06:01\t620.9\nKL888\t2016-06-24 06:02\t620.9\nKL888\t2016-06-24 06:03\t620.9\nKL888\t2016-06-24 06:04\t615.9333333333333\nKL888\t2016-06-24 06:05\t613.825\nKL888\t2016-06-24 06:06\t613.825\nKL888\t2016-06-24 06:07\t613.825\nKL888\t2016-06-24 06:08\t567.5625\nKL888\t2016-06-24 06:09\t610.1999999999999\nKL888\t2016-06-24 06:10\t610.1999999999999\nKL888\t2016-06-24 06:11\t610.1999999999999\nKL888\t2016-06-24 06:12\t610.1999999999999\nKL888\t2016-06-24 06:14\t605.9916666666667\nKL888\t2016-06-24 06:15\t605.9916666666667\nKL888\t2016-06-24 06:16\t605.9916666666667\nKL888\t2016-06-24 06:17\t603.3999999999999\nKL888\t2016-06-24 06:18\t597.1666666666667\nKL888\t2016-06-24 06:19\t601.4166666666666\nKL888\t2016-06-24 06:20\t599.7416666666667\nKL888\t2016-06-24 06:21\t599.7416666666667\nKL888\t2016-06-24 06:22\t599.7416666666667\nKL888\t2016-06-24 06:23\t597.1666666666667\nKL888\t2016-06-24 06:24\t583.1333333333333\nKL888\t2016-06-24 06:25\t583.1333333333333\nKL888\t2016-06-24 06:26\t583.1333333333333\nKL888\t2016-06-24 06:27\t583.1333333333333\nKL888\t2016-06-24 06:28\t579.1555555555555\nKL888\t2016-06-24 06:29\t579.1555555555555\nKL888\t2016-06-24 06:30\t579.1555555555555\nKL888\t2016-06-24 06:32\t579.1555555555555\nKL888\t2016-06-24 06:33\t573.95\nKL888\t2016-06-24 06:34\t573.95\nKL888\t2016-06-24 06:35\t573.95\nKL888\t2016-06-24 06:36\t573.95\nKL888\t2016-06-24 06:37\t573.95\nKL888\t2016-06-24 06:38\t573.95\nKL888\t2016-06-24 06:39\t579.1555555555555\nKL888\t2016-06-24 06:40\t579.1555555555555\nKL888\t2016-06-24 06:41\t579.1555555555555\nKL888\t2016-06-24 06:42\t579.1555555555555\nKL888\t2016-06-24 06:43\t579.1555555555555\nKL888\t2016-06-24 06:44\t579.1555555555555\nKL888\t2016-06-24 06:45\t579.1555555555555\nKL888\t2016-06-24 06:46\t583.1333333333333\nKL888\t2016-06-24 06:47\t583.1333333333333\nKL888\t2016-06-24 06:49\t583.1333333333333\nKL888\t2016-06-24 06:50\t583.1333333333333\nKL888\t2016-06-24 06:51\t583.1333333333333\nKL888\t2016-06-24 06:52\t579.1555555555555\nKL888\t2016-06-24 06:53\t540.5833333333334\nKL888\t2016-06-24 06:54\t583.1333333333333\nKL888\t2016-06-24 06:55\t567.5625\nKL888\t2016-06-24 06:56\t567.5625\nKL888\t2016-06-24 06:57\t567.5625\nKL888\t2016-06-24 06:58\t567.5625\nKL888\t2016-06-24 06:59\t567.5625\nKL888\t2016-06-24 07:00\t567.5625\nKL888\t2016-06-24 07:01\t567.5625\nKL888\t2016-06-24 07:02\t567.5625\nKL888\t2016-06-24 07:03\t567.5625\nKL888\t2016-06-24 07:04\t555.7666666666667\nKL888\t2016-06-24 07:05\t555.7666666666667\nKL888\t2016-06-24 07:06\t552.0833333333334\nKL888\t2016-06-24 07:08\t552.0833333333334\nKL888\t2016-06-24 07:09\t552.0833333333334\nKL888\t2016-06-24 07:10\t552.0833333333334\nKL888\t2016-06-24 07:11\t549.5166666666664\nKL888\t2016-06-24 07:12\t546.4083333333333\nKL888\t2016-06-24 07:13\t546.4083333333333\nKL888\t2016-06-24 07:14\t546.4083333333333\nKL888\t2016-06-24 07:15\t546.4083333333333\nKL888\t2016-06-24 07:16\t546.4083333333333\nKL888\t2016-06-24 07:17\t543.1999999999998\nKL888\t2016-06-24 07:18\t573.95\nKL888\t2016-06-24 07:19\t540.5833333333334\nKL888\t2016-06-24 07:20\t540.5833333333334\nKL888\t2016-06-24 07:21\t537.9666666666672\nKL888\t2016-06-24 07:22\t537.9666666666672\nKL888\t2016-06-24 07:23\t536.8999999999999\nKL888\t2016-06-24 07:24\t535.8666666666668\nKL888\t2016-06-24 07:25\t528.7933333333333\nKL888\t2016-06-24 07:26\t528.7933333333333\nKL888\t2016-06-24 07:27\t528.7933333333333\nKL888\t2016-06-24 07:28\t528.7933333333333\nKL888\t2016-06-24 07:29\t529.1722222222223\nKL888\t2016-06-24 07:31\t529.1722222222223\nKL888\t2016-06-24 07:31\t528.7933333333333\nKL888\t2016-06-24 07:33\t529.1722222222223\nKL888\t2016-06-24 07:34\t528.7933333333333\nKL888\t2016-06-24 07:35\t529.1722222222223\nKL888\t2016-06-24 07:36\t528.7933333333333\nKL888\t2016-06-24 07:37\t535.8666666666668\nKL888\t2016-06-24 07:38\t529.1722222222223\nKL888\t2016-06-24 07:39\t518.95\nKL888\t2016-06-24 07:40\t518.95\nKL888\t2016-06-24 07:41\t518.95\nKL888\t2016-06-24 07:42\t518.95\nKL888\t2016-06-24 07:43\t516.3083333333334\nKL888\t2016-06-24 07:44\t516.3083333333334\nKL888\t2016-06-24 07:46\t487.3833333333333\nKL888\t2016-06-24 07:47\t504.6266666666667\nKL888\t2016-06-24 07:48\t504.6266666666667\nKL888\t2016-06-24 07:49\t504.6266666666667\nKL888\t2016-06-24 07:50\t504.6266666666667\nKL888\t2016-06-24 07:51\t504.6266666666667\nKL888\t2016-06-24 07:52\t504.6266666666667\nKL888\t2016-06-24 07:53\t504.6266666666667\nKL888\t2016-06-24 07:54\t504.6266666666667\nKL888\t2016-06-24 07:55\t504.6266666666667\nKL888\t2016-06-24 07:56\t504.6266666666667\nKL888\t2016-06-24 07:57\t501.8055555555556\nKL888\t2016-06-24 07:58\t501.8055555555556\nKL888\t2016-06-24 07:59\t501.8055555555556\nKL888\t2016-06-24 08:00\t501.8055555555556\nKL888\t2016-06-24 08:01\t496.28888888888895\nKL888\t2016-06-24 08:03\t493.08333333333337\nKL888\t2016-06-24 08:03\t493.08333333333337\nKL888\t2016-06-24 08:05\t496.28888888888895\nKL888\t2016-06-24 08:06\t496.28888888888895\nKL888\t2016-06-24 08:07\t487.3833333333333\nKL888\t2016-06-24 08:07\t487.3833333333333\nKL888\t2016-06-24 08:09\t485.5666666666666\nKL888\t2016-06-24 08:10\t485.5666666666666\nKL888\t2016-06-24 08:11\t485.5666666666666\nKL888\t2016-06-24 08:12\t493.08333333333337\nKL888\t2016-06-24 08:13\t504.6266666666667\nKL888\t2016-06-24 08:14\t504.6266666666667\nKL888\t2016-06-24 08:15\t504.6266666666667\nKL888\t2016-06-24 08:16\t487.3833333333333\nKL888\t2016-06-24 08:17\t487.3833333333333\nKL888\t2016-06-24 08:18\t487.3833333333333\nKL888\t2016-06-24 08:19\t485.5666666666666\nKL888\t2016-06-24 08:20\t478.3499999999999\nKL888\t2016-06-24 08:21\t478.35\nKL888\t2016-06-24 08:22\t478.35\nKL888\t2016-06-24 08:23\t478.3499999999999\nKL888\t2016-06-24 08:24\t485.5666666666666\nKL888\t2016-06-24 08:26\t474.61666666666656\nKL888\t2016-06-24 08:27\t463.9777777777777\nKL888\t2016-06-24 08:28\t463.9777777777777\nKL888\t2016-06-24 08:29\t461.847619047619\nKL888\t2016-06-24 08:30\t463.9777777777777\nKL888\t2016-06-24 08:31\t467.1611111111112\nKL888\t2016-06-24 08:32\t467.1611111111112\nKL888\t2016-06-24 08:33\t467.1611111111112\nKL888\t2016-06-24 08:34\t467.1611111111112\nKL888\t2016-06-24 08:35\t463.9777777777777\nKL888\t2016-06-24 08:36\t463.9777777777777\nKL888\t2016-06-24 08:37\t461.847619047619\nKL888\t2016-06-24 08:38\t461.847619047619\nKL888\t2016-06-24 08:39\t461.847619047619\nKL888\t2016-06-24 08:40\t461.847619047619\nKL888\t2016-06-24 08:42\t461.847619047619\nKL888\t2016-06-24 08:42\t461.847619047619\nKL888\t2016-06-24 08:44\t463.9777777777777\nKL888\t2016-06-24 08:45\t463.9777777777777\nKL888\t2016-06-24 08:46\t463.9777777777777\nKL888\t2016-06-24 08:47\t447.4333333333331\nKL888\t2016-06-24 08:48\t447.4333333333331\nKL888\t2016-06-24 08:49\t447.4333333333331\nKL888\t2016-06-24 08:50\t447.4333333333331\nKL888\t2016-06-24 08:51\t447.4333333333331\nKL888\t2016-06-24 08:52\t447.4333333333331\nKL888\t2016-06-24 08:53\t447.4333333333331\nKL888\t2016-06-24 08:54\t447.4333333333331\nKL888\t2016-06-24 08:55\t447.4333333333331\nKL888\t2016-06-24 08:56\t447.4333333333331\nKL888\t2016-06-24 08:57\t447.4333333333331\nKL888\t2016-06-24 08:58\t435.5479166666666\nKL888\t2016-06-24 08:59\t435.5479166666666\nKL888\t2016-06-24 09:00\t435.5479166666666\nKL888\t2016-06-24 09:02\t435.5479166666666\nKL888\t2016-06-24 09:03\t435.5479166666666\nKL888\t2016-06-24 09:04\t435.5479166666666\nKL888\t2016-06-24 09:05\t435.5479166666666\nKL888\t2016-06-24 09:06\t435.5479166666666\nKL888\t2016-06-24 09:07\t435.5479166666666\nKL888\t2016-06-24 09:08\t429.91666666666663\nKL888\t2016-06-24 09:09\t429.91666666666663\nKL888\t2016-06-24 09:10\t429.91666666666663\nKL888\t2016-06-24 09:11\t428.64166666666665\nKL888\t2016-06-24 09:12\t428.64166666666665\nKL888\t2016-06-24 09:13\t429.91666666666663\nKL888\t2016-06-24 09:14\t429.91666666666663\nKL888\t2016-06-24 09:15\t424.96666666666664\nKL888\t2016-06-24 09:16\t421.7666666666667\nKL888\t2016-06-24 09:17\t421.7666666666667\nKL888\t2016-06-24 09:18\t421.7666666666667\nKL888\t2016-06-24 09:19\t421.7666666666667\nKL888\t2016-06-24 09:20\t435.5479166666666\nKL888\t2016-06-24 09:22\t409.1188888888888\nKL888\t2016-06-24 09:23\t409.1188888888888\nKL888\t2016-06-24 09:24\t409.1188888888888\nKL888\t2016-06-24 09:25\t409.1188888888888\nKL888\t2016-06-24 09:26\t409.1188888888888\nKL888\t2016-06-24 09:27\t409.1188888888888\nKL888\t2016-06-24 09:28\t409.1188888888888\nKL888\t2016-06-24 09:29\t409.1188888888888\nKL888\t2016-06-24 09:30\t409.1188888888888\nKL888\t2016-06-24 09:31\t409.1188888888888\nKL888\t2016-06-24 09:32\t409.1188888888888\nKL888\t2016-06-24 09:33\t409.1188888888888\nKL888\t2016-06-24 09:34\t409.1188888888888\nKL888\t2016-06-24 09:35\t409.1188888888888\nKL888\t2016-06-24 09:36\t409.1188888888888\nKL888\t2016-06-24 09:37\t409.1188888888888\nKL888\t2016-06-24 09:38\t409.1188888888888\nKL888\t2016-06-24 09:40\t409.1188888888888\nKL888\t2016-06-24 09:41\t409.1188888888888\nKL888\t2016-06-24 09:42\t409.1188888888888\nKL888\t2016-06-24 09:43\t409.1188888888888\nKL888\t2016-06-24 09:44\t409.1188888888888\nKL888\t2016-06-24 09:45\t409.1188888888888\nKL888\t2016-06-24 09:46\t409.1188888888888\nKL888\t2016-06-24 09:47\t391.8666666666667\nKL888\t2016-06-24 09:48\t391.8666666666667\nKL888\t2016-06-24 09:49\t388.8166666666667\nKL888\t2016-06-24 09:50\t388.8166666666667\nKL888\t2016-06-24 09:51\t388.8166666666667\nKL888\t2016-06-24 09:52\t388.8166666666667\nKL888\t2016-06-24 09:53\t388.8166666666667\nKL888\t2016-06-24 09:54\t388.8166666666667\nKL888\t2016-06-24 09:56\t384.63333333333344\nKL888\t2016-06-24 09:57\t383.5666666666666\nKL888\t2016-06-24 09:58\t382.55\nKL888\t2016-06-24 09:59\t380.9666666666667\nKL888\t2016-06-24 10:00\t380.9666666666667\nKL888\t2016-06-24 10:01\t379.43333333333334\nKL888\t2016-06-24 10:02\t347.16666666666674\nKL888\t2016-06-24 10:03\t376.6500000000001\nKL888\t2016-06-24 10:03\t362.5888888888889\nKL888\t2016-06-24 10:05\t376.6500000000001\nKL888\t2016-06-24 10:06\t362.5888888888889\nKL888\t2016-06-24 10:07\t362.5888888888889\nKL888\t2016-06-24 10:08\t367.9083333333333\nKL888\t2016-06-24 10:09\t367.9083333333333\nKL888\t2016-06-24 10:10\t367.9083333333333\nKL888\t2016-06-24 10:11\t367.9083333333333\nKL888\t2016-06-24 10:12\t362.5888888888889\nKL888\t2016-06-24 10:13\t367.9083333333333\nKL888\t2016-06-24 10:14\t362.5888888888889\nKL888\t2016-06-24 10:15\t364.68333333333334\nKL888\t2016-06-24 10:16\t363.68333333333334\nKL888\t2016-06-24 10:18\t362.5888888888889\nKL888\t2016-06-24 10:19\t362.5888888888889\nKL888\t2016-06-24 10:20\t362.5888888888889\nKL888\t2016-06-24 10:21\t362.5888888888889\nKL888\t2016-06-24 10:22\t362.5888888888889\nKL888\t2016-06-24 10:23\t362.5888888888889\nKL888\t2016-06-24 10:24\t362.5888888888889\nKL888\t2016-06-24 10:25\t355.18333333333334\nKL888\t2016-06-24 10:26\t353.03333333333285\nKL888\t2016-06-24 10:27\t353.03333333333285\nKL888\t2016-06-24 10:28\t352.01666666666665\nKL888\t2016-06-24 10:29\t352.01666666666665\nKL888\t2016-06-24 10:30\t353.03333333333285\nKL888\t2016-06-24 10:31\t348.78333333333336\nKL888\t2016-06-24 10:32\t348.78333333333336\nKL888\t2016-06-24 10:34\t346.4666666666667\nKL888\t2016-06-24 10:35\t345.4333333333334\nKL888\t2016-06-24 10:36\t345.4333333333334\nKL888\t2016-06-24 10:37\t342.725\nKL888\t2016-06-24 10:38\t342.725\nKL888\t2016-06-24 10:39\t337.13750000000005\nKL888\t2016-06-24 10:40\t335.82916666666665\nKL888\t2016-06-24 10:41\t335.82916666666665\nKL888\t2016-06-24 10:42\t335.82916666666665\nKL888\t2016-06-24 10:43\t337.13750000000005\nKL888\t2016-06-24 10:44\t337.13750000000005\nKL888\t2016-06-24 10:45\t337.13750000000005\nKL888\t2016-06-24 10:46\t333.75\nKL888\t2016-06-24 10:47\t335.82916666666665\nKL888\t2016-06-24 10:48\t335.82916666666665\nKL888\t2016-06-24 10:50\t319.1555555555555\nKL888\t2016-06-24 10:51\t334.4666666666667\nKL888\t2016-06-24 10:52\t323.11333333333334\nKL888\t2016-06-24 10:53\t327.4833333333336\nKL888\t2016-06-24 10:54\t323.11333333333334\nKL888\t2016-06-24 10:55\t323.11333333333334\nKL888\t2016-06-24 10:56\t323.11333333333334\nKL888\t2016-06-24 10:57\t323.11333333333334\nKL888\t2016-06-24 10:58\t323.11333333333334\nKL888\t2016-06-24 10:59\t323.11333333333334\nKL888\t2016-06-24 11:00\t323.11333333333334\nKL888\t2016-06-24 11:01\t317.54999999999995\nKL888\t2016-06-24 11:02\t317.54999999999995\nKL888\t2016-06-24 11:03\t317.54999999999995\nKL888\t2016-06-24 11:04\t317.54999999999995\nKL888\t2016-06-24 11:05\t319.1555555555555\nKL888\t2016-06-24 11:06\t319.1555555555555\nKL888\t2016-06-24 11:07\t319.1555555555555\nKL888\t2016-06-24 11:08\t311.19166666666666\nKL888\t2016-06-24 11:09\t311.19166666666666\nKL888\t2016-06-24 11:10\t311.19166666666666\nKL888\t2016-06-24 11:12\t308.6\nKL888\t2016-06-24 11:13\t308.6\nKL888\t2016-06-24 11:14\t305.8833333333333\nKL888\t2016-06-24 11:15\t305.8833333333333\nKL888\t2016-06-24 11:16\t304.29999999999995\nKL888\t2016-06-24 11:17\t302.0666666666666\nKL888\t2016-06-24 11:18\t302.0666666666666\nKL888\t2016-06-24 11:19\t295.68333333333334\nKL888\t2016-06-24 11:20\t295.68333333333334\nKL888\t2016-06-24 11:21\t298.8666666666666\nKL888\t2016-06-24 11:22\t298.8666666666666\nKL888\t2016-06-24 11:23\t298.8666666666666\nKL888\t2016-06-24 11:24\t295.68333333333334\nKL888\t2016-06-24 11:26\t289.95\nKL888\t2016-06-24 11:27\t289.2388888888889\nKL888\t2016-06-24 11:28\t281.2055555555556\nKL888\t2016-06-24 11:29\t289.2388888888889\nKL888\t2016-06-24 11:30\t289.2388888888889\nKL888\t2016-06-24 11:31\t289.95\nKL888\t2016-06-24 11:32\t302.0666666666666\nKL888\t2016-06-24 11:33\t289.95\nKL888\t2016-06-24 11:34\t289.95\nKL888\t2016-06-24 11:35\t285.0\nKL888\t2016-06-24 11:36\t289.2388888888889\nKL888\t2016-06-24 11:37\t281.2055555555556\nKL888\t2016-06-24 11:38\t281.2055555555556\nKL888\t2016-06-24 11:39\t281.2055555555556\nKL888\t2016-06-24 11:40\t281.2055555555556\nKL888\t2016-06-24 11:41\t281.2055555555556\nKL888\t2016-06-24 11:42\t277.225\nKL888\t2016-06-24 11:43\t277.225\nKL888\t2016-06-24 11:45\t275.61666666666656\nKL888\t2016-06-24 11:47\t308.6\nKL888\t2016-06-24 11:48\t272.3666666666668\nKL888\t2016-06-24 11:49\t246.95476190476188\nKL888\t2016-06-24 11:50\t267.7388888888889\nKL888\t2016-06-24 11:51\t269.1333333333333\nKL888\t2016-06-24 11:52\t269.1333333333333\nKL888\t2016-06-24 11:53\t267.7388888888889\nKL888\t2016-06-24 11:54\t267.7388888888889\nKL888\t2016-06-24 11:55\t267.7388888888889\nKL888\t2016-06-24 11:56\t267.7388888888889\nKL888\t2016-06-24 11:57\t261.74444444444447\nKL888\t2016-06-24 11:58\t261.74444444444447\nKL888\t2016-06-24 11:59\t261.74444444444447\nKL888\t2016-06-24 12:01\t256.8833333333333\nKL888\t2016-06-24 12:02\t256.8833333333333\nKL888\t2016-06-24 12:03\t257.43333333333334\nKL888\t2016-06-24 12:04\t256.8833333333333\nKL888\t2016-06-24 12:05\t256.8833333333333\nKL888\t2016-06-24 12:06\t246.95476190476188\nKL888\t2016-06-24 12:07\t246.95476190476188\nKL888\t2016-06-24 12:08\t246.95476190476188\nKL888\t2016-06-24 12:09\t246.95476190476188\nKL888\t2016-06-24 12:10\t246.95476190476188\nKL888\t2016-06-24 12:11\t246.95476190476188\nKL888\t2016-06-24 12:12\t246.95476190476188\nKL888\t2016-06-24 12:13\t246.95476190476188\nKL888\t2016-06-24 12:15\t246.95476190476188\nKL888\t2016-06-24 12:16\t246.95476190476188\nKL888\t2016-06-24 12:17\t246.95476190476188\nKL888\t2016-06-24 12:18\t246.95476190476188\nKL888\t2016-06-24 12:19\t246.95476190476188\nKL888\t2016-06-24 12:20\t246.95476190476188\nKL888\t2016-06-24 12:21\t246.95476190476188\nKL888\t2016-06-24 12:22\t246.95476190476188\nKL888\t2016-06-24 12:23\t246.95476190476188\nKL888\t2016-06-24 12:24\t230.5583333333333\nKL888\t2016-06-24 12:25\t230.5583333333333\nKL888\t2016-06-24 12:26\t230.5583333333333\nKL888\t2016-06-24 12:27\t229.35\nKL888\t2016-06-24 12:28\t229.35\nKL888\t2016-06-24 12:29\t230.5583333333333\nKL888\t2016-06-24 12:30\t230.5583333333333\nKL888\t2016-06-24 12:31\t230.5583333333333\nKL888\t2016-06-24 12:32\t230.5583333333333\nKL888\t2016-06-24 12:33\t229.35\nKL888\t2016-06-24 12:34\t230.5583333333333\nKL888\t2016-06-24 12:36\t230.5583333333333\nKL888\t2016-06-24 12:37\t223.56666666666666\nKL888\t2016-06-24 12:38\t219.5233333333333\nKL888\t2016-06-24 12:39\t219.5233333333333\nKL888\t2016-06-24 12:40\t219.5233333333333\nKL888\t2016-06-24 12:41\t218.7833333333333\nKL888\t2016-06-24 12:42\t218.7833333333333\nKL888\t2016-06-24 12:43\t219.5233333333333\nKL888\t2016-06-24 12:44\t219.5233333333333\nKL888\t2016-06-24 12:45\t215.0666666666666\nKL888\t2016-06-24 12:46\t207.51666666666665\nKL888\t2016-06-24 12:47\t211.25833333333333\nKL888\t2016-06-24 12:48\t211.25833333333333\nKL888\t2016-06-24 12:49\t211.25833333333333\nKL888\t2016-06-24 12:50\t211.25833333333333\nKL888\t2016-06-24 12:52\t211.25833333333333\nKL888\t2016-06-24 12:53\t207.51666666666665\nKL888\t2016-06-24 12:54\t204.79999999999995\nKL888\t2016-06-24 12:55\t204.79999999999995\nKL888\t2016-06-24 12:56\t204.79999999999995\nKL888\t2016-06-24 12:57\t202.08333333333334\nKL888\t2016-06-24 12:58\t202.08333333333334\nKL888\t2016-06-24 12:59\t185.49047619047616\nKL888\t2016-06-24 13:00\t185.49047619047616\nKL888\t2016-06-24 13:01\t191.1625\nKL888\t2016-06-24 13:02\t191.1625\nKL888\t2016-06-24 13:03\t191.1625\nKL888\t2016-06-24 13:04\t191.1625\nKL888\t2016-06-24 13:06\t191.1625\nKL888\t2016-06-24 13:07\t191.1625\nKL888\t2016-06-24 13:08\t191.1625\nKL888\t2016-06-24 13:09\t191.1625\nKL888\t2016-06-24 13:10\t191.1625\nKL888\t2016-06-24 13:11\t191.1625\nKL888\t2016-06-24 13:12\t187.33333333333337\nKL888\t2016-06-24 13:13\t187.33333333333337\nKL888\t2016-06-24 13:14\t191.1625\nKL888\t2016-06-24 13:15\t191.1625\nKL888\t2016-06-24 13:16\t191.1625\nKL888\t2016-06-24 13:17\t191.1625\nKL888\t2016-06-24 13:18\t185.49047619047616\nKL888\t2016-06-24 13:19\t185.49047619047616\nKL888\t2016-06-24 13:20\t185.49047619047616\nKL888\t2016-06-24 13:21\t185.49047619047616\nKL888\t2016-06-24 13:22\t191.1625\nKL888\t2016-06-24 13:23\t185.49047619047616\nKL888\t2016-06-24 13:24\t185.49047619047616\nKL888\t2016-06-24 13:25\t191.16666666666674\nKL888\t2016-06-24 13:27\t171.39444444444442\nKL888\t2016-06-24 13:28\t171.39444444444442\nKL888\t2016-06-24 13:29\t169.8166666666666\nKL888\t2016-06-24 13:30\t169.8166666666666\nKL888\t2016-06-24 13:31\t169.8166666666666\nKL888\t2016-06-24 13:32\t171.39444444444442\nKL888\t2016-06-24 13:33\t171.39444444444442\nKL888\t2016-06-24 13:34\t166.98333333333332\nKL888\t2016-06-24 13:35\t164.41666666666669\nKL888\t2016-06-24 13:36\t164.41666666666669\nKL888\t2016-06-24 13:37\t161.7277777777778\nKL888\t2016-06-24 13:38\t161.7277777777778\nKL888\t2016-06-24 13:39\t161.7277777777778\nKL888\t2016-06-24 13:41\t159.53333333333333\nKL888\t2016-06-24 13:42\t156.32222222222222\nKL888\t2016-06-24 13:43\t156.32222222222222\nKL888\t2016-06-24 13:44\t156.32222222222222\nKL888\t2016-06-24 13:45\t156.32222222222222\nKL888\t2016-06-24 13:46\t154.2166666666667\nKL888\t2016-06-24 13:47\t149.86999999999998\nKL888\t2016-06-24 13:48\t149.86999999999998\nKL888\t2016-06-24 13:49\t149.86999999999998\nKL888\t2016-06-24 13:50\t148.98333333333335\nKL888\t2016-06-24 13:51\t148.98333333333335\nKL888\t2016-06-24 13:52\t149.86999999999998\nKL888\t2016-06-24 13:53\t149.86999999999998\nKL888\t2016-06-24 13:54\t149.86999999999998\nKL888\t2016-06-24 13:55\t143.2166666666667\nKL888\t2016-06-24 13:56\t143.2166666666667\nKL888\t2016-06-24 13:57\t143.2166666666667\nKL888\t2016-06-24 13:58\t143.2166666666667\nKL888\t2016-06-24 14:00\t143.2166666666667\nKL888\t2016-06-24 14:01\t139.45\nKL888\t2016-06-24 14:02\t139.45\nKL888\t2016-06-24 14:03\t131.42916666666667\nKL888\t2016-06-24 14:04\t131.42916666666667\nKL888\t2016-06-24 14:05\t131.42916666666667\nKL888\t2016-06-24 14:06\t125.8690476190476\nKL888\t2016-06-24 14:07\t125.8690476190476\nKL888\t2016-06-24 14:08\t125.8690476190476\nKL888\t2016-06-24 14:09\t125.8690476190476\nKL888\t2016-06-24 14:10\t125.8690476190476\nKL888\t2016-06-24 14:11\t125.8690476190476\nKL888\t2016-06-24 14:12\t127.20000000000002\nKL888\t2016-06-24 14:13\t127.20000000000002\nKL888\t2016-06-24 14:15\t127.20000000000002\nKL888\t2016-06-24 14:16\t125.8690476190476\nKL888\t2016-06-24 14:17\t125.8690476190476\nKL888\t2016-06-24 14:18\t125.8690476190476\nKL888\t2016-06-24 14:19\t125.8690476190476\nKL888\t2016-06-24 14:20\t125.8690476190476\nKL888\t2016-06-24 14:21\t125.8690476190476\nKL888\t2016-06-24 14:22\t125.8690476190476\nKL888\t2016-06-24 14:23\t131.42916666666667\nKL888\t2016-06-24 14:24\t114.34166666666667\nKL888\t2016-06-24 14:25\t114.34166666666667\nKL888\t2016-06-24 14:26\t114.34166666666667\nKL888\t2016-06-24 14:27\t112.75\nKL888\t2016-06-24 14:28\t110.14166666666668\nKL888\t2016-06-24 14:29\t110.14166666666668\nKL888\t2016-06-24 14:31\t110.14166666666668\nKL888\t2016-06-24 14:32\t110.14166666666668\nKL888\t2016-06-24 14:33\t105.78333333333335\nKL888\t2016-06-24 14:34\t105.78333333333335\nKL888\t2016-06-24 14:35\t105.78333333333335\nKL888\t2016-06-24 14:36\t105.78333333333335\nKL888\t2016-06-24 14:37\t103.35000000000002\nKL888\t2016-06-24 14:38\t102.28333333333333\nKL888\t2016-06-24 14:39\t101.2\nKL888\t2016-06-24 14:40\t98.35555555555555\nKL888\t2016-06-24 14:41\t98.35555555555555\nKL888\t2016-06-24 14:42\t98.35555555555555\nKL888\t2016-06-24 14:43\t98.35555555555555\nKL888\t2016-06-24 14:44\t98.35555555555555\nKL888\t2016-06-24 14:45\t98.62777777777778\nKL888\t2016-06-24 14:46\t87.8638888888889\nKL888\t2016-06-24 14:47\t88.41666666666663\nKL888\t2016-06-24 14:49\t87.8638888888889\nKL888\t2016-06-24 14:50\t87.8638888888889\nKL888\t2016-06-24 14:51\t88.41666666666663\nKL888\t2016-06-24 14:52\t88.41666666666663\nKL888\t2016-06-24 14:53\t87.8638888888889\nKL888\t2016-06-24 14:54\t87.8638888888889\nKL888\t2016-06-24 14:55\t87.8638888888889\nKL888\t2016-06-24 14:56\t87.8638888888889\nKL888\t2016-06-24 14:57\t79.03888888888889\nKL888\t2016-06-24 14:58\t79.03888888888889\nKL888\t2016-06-24 14:59\t79.03888888888889\nKL888\t2016-06-24 15:00\t79.03888888888889\nKL888\t2016-06-24 15:01\t79.03888888888889\nKL888\t2016-06-24 15:03\t79.03888888888889\nKL888\t2016-06-24 15:04\t79.03888888888889\nKL888\t2016-06-24 15:05\t79.03888888888889\nKL888\t2016-06-24 15:06\t72.0111111111111\nKL888\t2016-06-24 15:07\t72.0111111111111\nKL888\t2016-06-24 15:08\t72.0111111111111\nKL888\t2016-06-24 15:09\t71.3\nKL888\t2016-06-24 15:10\t72.0111111111111\nKL888\t2016-06-24 15:11\t69.18333333333337\nKL888\t2016-06-24 15:12\t68.13333333333334\nKL888\t2016-06-24 15:13\t68.13333333333334\nKL888\t2016-06-24 15:14\t66.06666666666666\nKL888\t2016-06-24 15:15\t62.09583333333333\nKL888\t2016-06-24 15:16\t62.09583333333333\nKL888\t2016-06-24 15:17\t62.09583333333333\nKL888\t2016-06-24 15:18\t62.09583333333333\nKL888\t2016-06-24 15:19\t62.09583333333333\nKL888\t2016-06-24 15:20\t62.09583333333333\nKL888\t2016-06-24 15:21\t57.575\nKL888\t2016-06-24 15:23\t57.575\nKL888\t2016-06-24 15:24\t57.575\nKL888\t2016-06-24 15:25\t57.575\nKL888\t2016-06-24 15:26\t54.4\nKL888\t2016-06-24 15:27\t49.050000000000004\nKL888\t2016-06-24 15:28\t49.050000000000004\nKL888\t2016-06-24 15:29\t49.050000000000004\nKL888\t2016-06-24 15:30\t49.050000000000004\nKL888\t2016-06-24 15:31\t49.050000000000004\nKL888\t2016-06-24 15:32\t49.050000000000004\nKL888\t2016-06-24 15:33\t42.56666666666667\nKL888\t2016-06-24 15:34\t45.83333333333334\nKL888\t2016-06-24 15:35\t42.56666666666667\nKL888\t2016-06-24 15:36\t43.666666666666664\nKL888\t2016-06-24 15:38\t42.56666666666667\nKL888\t2016-06-24 15:39\t40.4\nKL888\t2016-06-24 15:40\t40.4\nKL888\t2016-06-24 15:41\t39.3\nKL888\t2016-06-24 15:42\t39.03666666666667\nKL888\t2016-06-24 15:43\t39.03666666666667\nKL888\t2016-06-24 15:44\t35.048095238095236\nKL888\t2016-06-24 15:45\t35.048095238095236\nKL888\t2016-06-24 15:46\t35.37222222222223\nKL888\t2016-06-24 15:47\t35.37222222222223\nKL888\t2016-06-24 15:48\t35.37222222222223\nKL888\t2016-06-24 15:49\t29.850000000000005\nKL888\t2016-06-24 15:50\t29.850000000000005\nKL888\t2016-06-24 15:51\t28.783333333333335\nKL888\t2016-06-24 15:52\t25.47083333333333\nKL888\t2016-06-24 15:53\t26.733333333333338\nKL888\t2016-06-24 15:55\t25.84166666666667\nKL888\t2016-06-24 15:56\t24.405555555555555\nKL888\t2016-06-24 15:57\t24.405555555555555\nKL888\t2016-06-24 15:58\t22.46666666666667\nKL888\t2016-06-24 15:59\t21.46666666666667\nKL888\t2016-06-24 16:00\t20.604166666666664\nKL888\t2016-06-24 16:01\t19.629166666666666\nKL888\t2016-06-24 16:02\t18.299999999999997\nKL888\t2016-06-24 16:03\t16.240740740740744\nKL888\t2016-06-24 16:04\t16.240740740740744\nKL888\t2016-06-24 16:05\t14.260416666666666\nKL888\t2016-06-24 16:06\t14.260416666666666\nKL888\t2016-06-24 16:07\t14.260416666666666\nKL888\t2016-06-24 16:08\t10.841666666666667\nKL888\t2016-06-24 16:09\t10.841666666666667\nKL888\t2016-06-24 16:10\t10.233333333333334\nKL888\t2016-06-24 16:11\t7.383333333333334\nKL888\t2016-06-24 16:13\t9.383333333333335\nKL888\t2016-06-24 16:14\t6.316666666666666\nKL888\t2016-06-24 16:15\t5.45\nKL888\t2016-06-24 16:16\t4.012820512820513\nKL888\t2016-06-24 16:17\t2.941666666666666\nKL888\t2016-06-24 16:18\t1.3066666666666666\nKL888\t2016-06-24 16:19\t0.0\nKL888\t2016-06-24 16:20\t0.0\nKL888\t2016-06-24 16:21\t0.0\nKL888\t2016-06-24 16:22\t0.0\nKL888\t2016-06-24 16:24\t0.0\nKL888\t2016-06-24 16:25\t0.0\nKL888\t2016-06-24 16:26\t0.0\nKL888\t2016-06-24 16:27\t0.0\nKL888\t2016-06-24 16:28\t0.0\nKL888\t2016-06-24 16:29\t0.0\nKL888\t2016-06-24 16:30\t0.0\nKL888\t2016-06-24 16:30\t0.0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517747049012_1878720477",
      "id": "20180204-122409_427847771",
      "dateCreated": "Feb 4, 2018 12:24:09 PM",
      "dateStarted": "Feb 4, 2018 12:28:03 PM",
      "dateFinished": "Feb 4, 2018 12:28:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\n\nquery.stop()",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:29:01 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1517168631076_-796771430",
      "id": "20180128-194351_1986237421",
      "dateCreated": "Jan 28, 2018 7:43:51 PM",
      "dateStarted": "Feb 4, 2018 12:29:01 PM",
      "dateFinished": "Feb 4, 2018 12:29:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n### For future usage show some samples using online ML predictions in SkLearn\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:45:14 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFor future usage show some samples using online ML predictions in SkLearn\u003c/h3\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517747144568_-296311027",
      "id": "20180204-122544_1087977051",
      "dateCreated": "Feb 4, 2018 12:25:44 PM",
      "dateStarted": "Feb 4, 2018 12:45:14 PM",
      "dateFinished": "Feb 4, 2018 12:45:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n\npip install sklearn\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:48:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Collecting sklearn\n  Downloading sklearn-0.0.tar.gz\nCollecting scikit-learn (from sklearn)\n  Downloading scikit_learn-0.19.1-cp34-cp34m-manylinux1_x86_64.whl (12.4MB)\nInstalling collected packages: scikit-learn, sklearn\n  Running setup.py install for sklearn: started\n    Running setup.py install for sklearn: finished with status \u0027done\u0027\nSuccessfully installed scikit-learn-0.19.1 sklearn-0.0\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748314257_-2112091040",
      "id": "20180204-124514_2016063602",
      "dateCreated": "Feb 4, 2018 12:45:14 PM",
      "dateStarted": "Feb 4, 2018 12:48:37 PM",
      "dateFinished": "Feb 4, 2018 12:48:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create some model on iris data",
      "text": "%pyspark\n\nimport numpy as np\nfrom sklearn import datasets\niris \u003d datasets.load_iris()\niris_X \u003d iris.data\niris_y \u003d iris.target  # Set of 0 , 1 and 2\u0027s \n#np.unique(iris_y)\n\n# Split iris data in train and test data\n# A random permutation, to split the data randomly\nnp.random.seed(0)\nindices \u003d np.random.permutation(len(iris_X))\niris_X_train \u003d iris_X[indices[:-10]]\niris_y_train \u003d iris_y[indices[:-10]]\niris_X_test  \u003d iris_X[indices[-10:]]\niris_y_test  \u003d iris_y[indices[-10:]]\n# Create and fit a nearest-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn \u003d KNeighborsClassifier()\n#nsamples, nx, ny \u003d iris_X_train.shape\n#iris_X_train \u003d iris_X_train.reshape((nsamples,nx*ny))\n#nsamples, nx, ny \u003d iris_y_train.shape\n#iris_y_train \u003d iris_y_train.reshape((nsamples,nx*ny))\n\nknn.fit(iris_X_train, iris_y_train) \nKNeighborsClassifier(algorithm\u003d\u0027auto\u0027, leaf_size\u003d30, metric\u003d\u0027minkowski\u0027,\n           metric_params\u003dNone, n_jobs\u003d1, n_neighbors\u003d5, p\u003d2,\n           weights\u003d\u0027uniform\u0027)\nknn.predict(iris_X_test)\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:49:43 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "array([1, 2, 1, 0, 0, 0, 2, 1, 2, 0])\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748517615_-1512592601",
      "id": "20180204-124837_802717680",
      "dateCreated": "Feb 4, 2018 12:48:37 PM",
      "dateStarted": "Feb 4, 2018 12:49:41 PM",
      "dateFinished": "Feb 4, 2018 12:49:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Spark batch prediction using sklearn",
      "text": "%pyspark\n\n#https://stackoverflow.com/questions/42887621/how-to-do-prediction-with-sklearn-model-inside-spark\n\nrdd \u003d sc.parallelize([[[ 7.7,  3.8,  6.7,  2.2]]]) #1 entry \n#rdd \u003d sc.parallelize([[[ 7.7,  3.8,  6.7,  2.2],[ 5.5,  2.6,  4.4,  1.2]]]) #multiple entries \n\nregr_bc \u003d sc.broadcast(knn) #broadcast model to workers\n\npredictions_list \u003d rdd.map(lambda x: (x, regr_bc.value.predict(x))).collect()\n\n#show prediction\nprint(predictions_list)\nfor x,predictions in predictions_list:\n    for count in range(0,len(predictions)):\n        print (\"input set \" + str(x[count]) + \" gives prediction : \" + str(predictions[count]))\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:50:34 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {},
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[([[7.7, 3.8, 6.7, 2.2]], array([2]))]\ninput set [7.7, 3.8, 6.7, 2.2] gives prediction : 2\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748581656_1336011634",
      "id": "20180204-124941_423898452",
      "dateCreated": "Feb 4, 2018 12:49:41 PM",
      "dateStarted": "Feb 4, 2018 12:50:34 PM",
      "dateFinished": "Feb 4, 2018 12:50:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create file with entry so we can test spark streaming",
      "text": "%sh\n\nmkdir /tmp/test\necho [[[ 7.7,  3.8,  6.7,  2.2]]] \u003e /tmp/test/input.csv\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:51:38 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1517748634703_-2068353903",
      "id": "20180204-125034_1509285486",
      "dateCreated": "Feb 4, 2018 12:50:34 PM",
      "dateStarted": "Feb 4, 2018 12:51:38 PM",
      "dateFinished": "Feb 4, 2018 12:51:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Test if this file can be used by spark batch",
      "text": "%pyspark\n\n#Yes it can be used but requires literal loading of the array, otherwise\n#it will be seen as string\nimport ast\n\nfrom pyspark.sql.types import StructType\nimport json\n\nuserSchema \u003d StructType().add(\"value\", \"string\")\n#Use non , delimter as we want whole record\nlines \u003d spark.read.format(\"csv\").option(\"delimiter\",\"|\").schema(userSchema).load(\"/tmp/test\")\nrdd \u003d sc.parallelize( ast.literal_eval( lines.first()[\"value\"] ) )\n\nrdd.map(lambda x: (x, regr_bc.value.predict(x))).collect()\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:53:29 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "[([[7.7, 3.8, 6.7, 2.2]], array([2]))]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748698731_-1610225232",
      "id": "20180204-125138_1235395620",
      "dateCreated": "Feb 4, 2018 12:51:38 PM",
      "dateStarted": "Feb 4, 2018 12:52:59 PM",
      "dateFinished": "Feb 4, 2018 12:53:00 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Start and test spark streaming (structured) without prediction",
      "text": "%pyspark\r\n\r\nfrom pyspark.sql.types import StructType\r\n\r\nuserSchema \u003d StructType().add(\"value\", \"string\")\r\nlines \u003d spark.readStream.format(\"csv\").option(\"delimiter\",\"|\").schema(userSchema).load(\"/tmp/test\")\r\n\r\ncounts \u003d lines.groupBy(\"value\").count()\r\n\r\n#rdd \u003d sc.parallelize( ast.literal_eval( lines.first()[\"value\"] ) )\r\n#lines \u003d lines.select ( lines.rdd.map(lambda x: (x, regr_bc.value.predict( x ))).collect() ).alias(\"prediction\")\r\n\r\n\r\nquery \u003d (counts\r\n  .writeStream\r\n  .outputMode(\"complete\")\r\n  .format(\"memory\")\r\n  .queryName(\"some_name\")\r\n  .start())",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:55:57 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1517748722695_2054554014",
      "id": "20180204-125202_1607993858",
      "dateCreated": "Feb 4, 2018 12:52:02 PM",
      "dateStarted": "Feb 4, 2018 12:54:04 PM",
      "dateFinished": "Feb 4, 2018 12:54:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\n\nselect * from some_name\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:54:23 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sql"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TABLE",
            "data": "value\tcount\n[[[ 7.7, 3.8, 6.7, 2.2]]]\t1\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748844396_-404052231",
      "id": "20180204-125404_1307507231",
      "dateCreated": "Feb 4, 2018 12:54:04 PM",
      "dateStarted": "Feb 4, 2018 12:54:23 PM",
      "dateFinished": "Feb 4, 2018 12:54:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "add new data (run query again afterwards)",
      "text": "%sh\n\nls /tmp/test\ncp /tmp/test/input.csv /tmp/test/inputme.csv\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:57:37 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/sh",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "input.csv\ninputmd.csv\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748882397_-419949529",
      "id": "20180204-125442_1024157324",
      "dateCreated": "Feb 4, 2018 12:54:42 PM",
      "dateStarted": "Feb 4, 2018 12:57:37 PM",
      "dateFinished": "Feb 4, 2018 12:57:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Use spark streaming to predict (add new file within 20 seconds! to show result)",
      "text": "%pyspark\n\nimport ast\nfrom pyspark.streaming import StreamingContext\n\nssc \u003d StreamingContext(sc, 2)\n \nlines \u003d ssc.textFileStream(\"/tmp/test\")\n\ndef process_rdd(rdd):\n    rdd_transformed \u003d rdd.map(lambda x: (x, regr_bc.value.predict( ast.literal_eval(x)[0])))\n    print (rdd_transformed.take(1))\n\ndef empty_rdd():\n    print (\"###The current RDD is empty. Wait for the next complete RDD ###\")\n    \n        \nlines.foreachRDD(lambda rdd: process_rdd(rdd) if rdd.count() \u003e 0 else empty_rdd())\n\n\nssc.start()\nssc.awaitTerminationOrTimeout(20)\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:57:31 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\n[(\u0027[[[ 7.7, 3.8, 6.7, 2.2]]]\u0027, array([2]))]\n###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\n###The current RDD is empty. Wait for the next complete RDD ###\nFalse\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1517748905070_1859348928",
      "id": "20180204-125505_1442852996",
      "dateCreated": "Feb 4, 2018 12:55:05 PM",
      "dateStarted": "Feb 4, 2018 12:57:31 PM",
      "dateFinished": "Feb 4, 2018 12:57:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Structured streaming does not work yet with sklearn",
      "text": "%pyspark\r\n\r\n#https://docs.databricks.com/spark/latest/structured-streaming/examples.html\r\n\r\nfrom pyspark.sql.functions import lit\r\n\r\nfrom pyspark.sql.types import StructType,IntegerType\r\nfrom pyspark.sql.types import *\r\n\r\n\r\nuserSchema \u003d StructType().add(\"value\", \"string\")\r\nlines \u003d spark.readStream.format(\"csv\").option(\"delimiter\",\"|\").schema(userSchema).load(\"/tmp/test\").selectExpr(\"CAST(value AS string)\")\r\n\r\n#rdd \u003d sc.parallelize( ast.literal_eval( lines.first()[\"value\"] ) )\r\n#pred \u003d rdd.map(lambda x: (x, regr_bc.value.predict(x))).collect()\r\n\r\nfrom pyspark.sql.functions import udf\r\n\r\n#lines.rdd.map(lambda x: (x, regr_bc.value.predict(x)))\r\n\r\n#predict_udf \u003d udf(regr_bc.value.predict, IntegerType())\r\n#predict_udf \u003d udf(regr_bc.value.predict, IntegerType())\r\n\r\nquery \u003d (lines\r\n  .writeStream\r\n  .format(\"memory\")\r\n  .queryName(\"some_pred6\")\r\n  .outputMode(\"append\")\r\n  .start())\r\n  \r\n#  .outputMode(\"append\")\r\n#  .trigger(processingTime\u003d\"2 seconds\")\r\n# .withColumn(\"prediction\",lines[\"value\"]  )\r\n#.withColumn(\"prediction\",  predict_udf(lines.value   ) )\r\n\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\r\n\r\nz.show(spark.table(\"some_pred6\"))\r\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:58:29 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1517749046421_-1375852976",
      "id": "20180204-125726_487284713",
      "dateCreated": "Feb 4, 2018 12:57:26 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n\nquery.stop()\n",
      "user": "anonymous",
      "dateUpdated": "Feb 4, 2018 12:58:59 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false
        },
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1517749130541_1171123446",
      "id": "20180204-125850_1252438679",
      "dateCreated": "Feb 4, 2018 12:58:50 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "flight prediction",
  "id": "2D48DXZYZ",
  "angularObjects": {
    "2D28D18EM:shared_process": [],
    "2D1KQV8RK:shared_process": [],
    "2CZM77WW6:shared_process": [],
    "2CY5ASEHH:shared_process": [],
    "2CZTG3WJA:shared_process": [],
    "2CZP9NCGW:shared_process": [],
    "2D23VCUHP:shared_process": [],
    "2CY3UAQ65:shared_process": [],
    "2CZ5FXHTZ:shared_process": []
  },
  "config": {},
  "info": {}
}